{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "191216TF_IPIP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhuang12/Tensorflow-for-personality-items-classification/blob/master/191216TF_IPIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52bYMFnQprcf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#THIS NOTEBOOK WAS CREATED TO USE TF NLP FOR IPIP ITEMS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkBl41YZNuzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HmQPn2QLInf",
        "colab_type": "code",
        "outputId": "8a7eddc1-9585-49f9-a88c-b1c62a1a82e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Di92YMtLRR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVnl38Ttp19e",
        "colab_type": "text"
      },
      "source": [
        "DATA LOADING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WLvGKL-p1Qx",
        "colab_type": "code",
        "outputId": "e374a64f-2617-4283-bf1d-d2cba0c6526e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWPpXW6IMU24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dpath = os.path.join(r\"C:/Users/amead/Google Drive/active/machine learning/big5/big_five_items.csv\")\n",
        "dpath = os.path.join(\"/content/drive/My Drive/big_five_items.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XVQVhbQMw-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(dpath,low_memory = False, sep = \"\\t\", lineterminator = '\\n', encoding = 'utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTEs9VUUO6RB",
        "colab_type": "text"
      },
      "source": [
        "DATA PROCESSING\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82XZ2o3yO_ib",
        "colab_type": "code",
        "outputId": "cde8e8e8-10a2-4ebf-c542-5f8814c1fc09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "#stem as one string\n",
        "sentences = pd.Series(data['Stem'])\n",
        "sentences"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                    I seldom feel blue.\n",
              "1        I am not interested in other people's problems.\n",
              "2                                  I carry out my plans.\n",
              "3                                 I make friends easily.\n",
              "4                       I am quick to understand things.\n",
              "                             ...                        \n",
              "650                       I'm pretty stable emotionally.\n",
              "651                                      I laugh easily.\n",
              "652    I believe that the \"new morality\" of a permiss...\n",
              "653    I would rather be known as \"merciful\" than as ...\n",
              "654            I think twice before I answer a question.\n",
              "Name: Stem, Length: 655, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3g2CgyHRu4t",
        "colab_type": "code",
        "outputId": "ab92e18b-846b-46cc-df42-42d1357dc25e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#scale as categorical data\n",
        "data['labels'] = data['Scale'].astype('category').cat.codes\n",
        "data.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>#ID</th>\n",
              "      <th>Scale Item ID</th>\n",
              "      <th>Scale</th>\n",
              "      <th>Direction</th>\n",
              "      <th>Stem</th>\n",
              "      <th>Instrument</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Neuroticism</td>\n",
              "      <td>-</td>\n",
              "      <td>I seldom feel blue.</td>\n",
              "      <td>BFAS</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Agreeableness</td>\n",
              "      <td>-</td>\n",
              "      <td>I am not interested in other people's problems.</td>\n",
              "      <td>BFAS</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>Conscientiousness</td>\n",
              "      <td>+</td>\n",
              "      <td>I carry out my plans.</td>\n",
              "      <td>BFAS</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>Extraversion</td>\n",
              "      <td>+</td>\n",
              "      <td>I make friends easily.</td>\n",
              "      <td>BFAS</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>Openness</td>\n",
              "      <td>+</td>\n",
              "      <td>I am quick to understand things.</td>\n",
              "      <td>BFAS</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   #ID Scale Item ID  ... Instrument labels\n",
              "0    1             1  ...       BFAS      3\n",
              "1    2             2  ...       BFAS      0\n",
              "2    3             3  ...       BFAS      1\n",
              "3    4             4  ...       BFAS      2\n",
              "4    5             5  ...       BFAS      4\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJJoFen1Uj03",
        "colab_type": "code",
        "outputId": "a6efe35b-fc4c-4608-b242-9e88a37de2d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "labels = np.array(pd.Series(data['labels']))\n",
        "labels"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 0, 1, 2, 4, 3, 0, 1, 2, 4, 3, 0, 1, 2, 4, 3, 0, 1, 2, 4, 3, 0,\n",
              "       1, 2, 4, 3, 0, 1, 2, 4, 3, 0, 1, 2, 4, 3, 0, 1, 2, 4, 3, 0, 1, 2,\n",
              "       4, 3, 0, 1, 2, 4, 3, 0, 1, 2, 4, 3, 0, 1, 2, 4, 3, 0, 1, 2, 4, 3,\n",
              "       0, 1, 2, 4, 3, 0, 1, 2, 4, 3, 0, 1, 2, 4, 3, 0, 1, 2, 4, 3, 0, 1,\n",
              "       2, 4, 3, 0, 1, 2, 4, 3, 0, 1, 2, 4, 2, 0, 1, 3, 4, 2, 0, 1, 3, 4,\n",
              "       2, 0, 1, 3, 4, 2, 0, 1, 3, 4, 2, 0, 1, 3, 4, 2, 0, 1, 3, 4, 2, 0,\n",
              "       1, 3, 4, 2, 0, 1, 3, 4, 4, 0, 1, 4, 4, 2, 2, 0, 2, 0, 3, 4, 4, 2,\n",
              "       3, 2, 2, 2, 3, 2, 4, 1, 3, 1, 1, 0, 0, 1, 4, 1, 3, 0, 0, 0, 4, 4,\n",
              "       4, 0, 2, 3, 4, 0, 1, 0, 1, 0, 3, 1, 2, 3, 3, 1, 2, 2, 3, 0, 2, 1,\n",
              "       3, 3, 4, 0, 1, 1, 2, 0, 4, 0, 2, 1, 3, 0, 4, 1, 4, 1, 3, 1, 3, 1,\n",
              "       3, 1, 1, 1, 2, 4, 4, 3, 3, 1, 4, 0, 4, 2, 2, 3, 4, 3, 3, 0, 0, 2,\n",
              "       3, 1, 3, 1, 1, 1, 3, 1, 2, 0, 4, 0, 2, 1, 4, 1, 2, 0, 2, 3, 4, 2,\n",
              "       3, 0, 2, 0, 4, 3, 4, 4, 4, 0, 2, 4, 4, 0, 1, 1, 4, 0, 3, 0, 1, 2,\n",
              "       3, 1, 2, 3, 0, 2, 0, 0, 4, 1, 3, 3, 3, 3, 4, 4, 4, 4, 0, 4, 2, 4,\n",
              "       0, 1, 2, 2, 2, 0, 2, 1, 0, 3, 4, 3, 2, 2, 0, 0, 0, 2, 0, 3, 1, 2,\n",
              "       3, 1, 1, 1, 4, 4, 0, 4, 4, 4, 4, 1, 1, 2, 4, 4, 1, 1, 2, 2, 0, 0,\n",
              "       0, 0, 0, 1, 1, 2, 1, 2, 2, 0, 4, 0, 2, 2, 0, 3, 2, 3, 3, 1, 1, 2,\n",
              "       3, 3, 3, 3, 3, 2, 4, 3, 4, 2, 4, 1, 3, 3, 4, 3, 1, 0, 4, 0, 1, 0,\n",
              "       4, 4, 0, 4, 3, 0, 1, 3, 4, 4, 2, 2, 0, 3, 2, 1, 2, 1, 3, 3, 2, 4,\n",
              "       0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1,\n",
              "       3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2,\n",
              "       4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0,\n",
              "       1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3,\n",
              "       2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4,\n",
              "       0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1,\n",
              "       3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2,\n",
              "       4, 0, 1, 3, 2, 4, 0, 0, 1, 3, 2, 4, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0,\n",
              "       1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3,\n",
              "       2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4,\n",
              "       0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1, 3, 2, 4, 0, 1], dtype=int8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp0mlKadp8x8",
        "colab_type": "text"
      },
      "source": [
        "DATA CLEANING (STEMMING)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3YwSO6_p_-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjR16yfrWvZ5",
        "colab_type": "text"
      },
      "source": [
        "TOKENIZATION AND PADDING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RahwW-DvWucE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# tokenizer = Tokenizer(oov_token = \"<OOV>\", num_words = 10000)\n",
        "# tokenizer.fit_on_texts(sentences)\n",
        "# word_index = tokenizer.word_index\n",
        "# print(word_index)\n",
        "\n",
        "# sequences = tokenizer.texts_to_sequences(sentences)\n",
        "# padded = pad_sequences(sequences, padding = 'pre')\n",
        "# print(padded[0])\n",
        "# print(padded.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzQw4-FfqDGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split training and validation sample\n",
        "#training_data, training_labels = np.asarray(padded[ :524]), labels[ :524]\n",
        "#test_data, test_labels = np.asarray(padded[525: ]), labels[525: ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPRZsKqNqAzI",
        "colab_type": "text"
      },
      "source": [
        "TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djKwOT6_mPVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random \n",
        "random.seed(700)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mjsXQIDLp3WR",
        "colab": {}
      },
      "source": [
        "#stratified random sampling training and validation sample by dimensions\n",
        "\n",
        "training_data = []\n",
        "training_labels = []\n",
        "validation_data = []\n",
        "validation_labels = []\n",
        "\n",
        "for s in range(5):\n",
        "  #print(s)\n",
        "  training_set = data.loc[data['labels'] == s, 'Stem'].sample(frac = 0.7, replace = False, random_state = 1)\n",
        "  training_l = np.array([s]*len(training_set))\n",
        "\n",
        "  validation_set = data.loc[(~data['Stem'].isin(training_set)) & (data['labels'] == s), 'Stem']\n",
        "  validation_l = np.array([s]*len(validation_set))\n",
        "\n",
        "  #print(len(training_set))\n",
        "  #print(len(validation_set))\n",
        "\n",
        "  training_data.extend(training_set)\n",
        "  training_labels.extend(training_l)\n",
        "\n",
        "  validation_data.extend(validation_set)\n",
        "  validation_labels.extend(validation_l)\n",
        "\n",
        "  #print(len(training_data))\n",
        "  #print(len(validation_data))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySUmrKW22KE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LENGTH = 10000\n",
        "\n",
        "def sequence_vectorization (train_texts, val_texts): \n",
        "  \"\"\"Vectorizes texts as sequence vectors.\n",
        "\n",
        "    1 text = 1 sequence vector with fixed length.\n",
        "\n",
        "    # Arguments\n",
        "        train_texts: list, training text strings.\n",
        "        val_texts: list, validation text strings.\n",
        "\n",
        "    # Returns\n",
        "        x_train, x_val, word_index: vectorized training and validation\n",
        "            texts and word index dictionary.\n",
        "  \"\"\"\n",
        "  tokenizer = Tokenizer(oov_token = \"<OOV>\", num_words = MAX_LENGTH)\n",
        "  # Create vocabulary with training texts.\n",
        "  tokenizer.fit_on_texts(train_texts)\n",
        "  x_train = tokenizer.texts_to_sequences(train_texts)\n",
        "  x_val = tokenizer.texts_to_sequences(val_texts)\n",
        "  \n",
        "  # Get max sequence length.\n",
        "  max_length = len(max(x_train, key=len))\n",
        "  \n",
        "  if max_length > MAX_LENGTH:\n",
        "    max_length = MAX_LENGTH\n",
        " \n",
        "# Fix sequence length to max value. Sequences shorter than the length are\n",
        "# padded in the beginning and sequences longer are truncated\n",
        "# at the beginning.\n",
        "  x_train = np.asarray(pad_sequences(x_train, maxlen=max_length))\n",
        "  x_val = np.asarray(pad_sequences(x_val, maxlen=max_length))\n",
        "\n",
        "  return x_train, x_val, tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUCHHm7xj2p_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_vec, val_vec, word_index = sequence_vectorization(training_data, validation_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r106kAqj6Roh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_labels = np.array(training_labels)\n",
        "validation_labels = np.array(validation_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1KVlwVE1qzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#summary of tokenization - validation data\n",
        "# print(tokenizer.word_counts)\n",
        "# print(tokenizer.document_count)\n",
        "# print(tokenizer.word_index)\n",
        "# print(tokenizer.word_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC-s3jc7qH8Q",
        "colab_type": "text"
      },
      "source": [
        "MODEL BUILING - MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pPj0LZjZkXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ACCURACY_STOP = 0.95\n",
        "\n",
        "class myCallbacks(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs = {}):\n",
        "    if (logs.get('accuracy')> ACCURACY_STOP):\n",
        "      self.model.stop_training = True\n",
        "      print(\"\\nReach accuracy of 95% and stop training!\")\n",
        "\n",
        "callbacks = myCallbacks()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGnjMaSYn1V2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_directory = '/content/drive/My Drive/NLP_tensorflow/sequentia_model.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(checkpoint_directory,\n",
        "                             monitor = \"val_accuracy\", \n",
        "                             save_best_only = True, \n",
        "                             mode = 'max')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-x9yGNwAUyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_last_layer_units_and_activation(num_classes):\n",
        "    \"\"\"Gets the # units and activation function for the last network layer.\n",
        "\n",
        "    # Arguments\n",
        "        num_classes: int, number of classes.\n",
        "\n",
        "    # Returns\n",
        "        units, activation values.\n",
        "    \"\"\"\n",
        "    if num_classes == 2:\n",
        "        activation = 'sigmoid'\n",
        "        units = 1\n",
        "    else:\n",
        "        activation = 'softmax'\n",
        "        units = num_classes\n",
        "    return units, activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y_7yzwQHI_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4IqBtVJAkOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mlp_model(layers, units, dropout_rate, num_classes):\n",
        "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
        "\n",
        "    # Arguments\n",
        "        layers: int, number of `Dense` layers in the model.\n",
        "        units: int, output dimension of the layers.\n",
        "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
        "        input_shape: tuple, shape of input to the model.\n",
        "        num_classes: int, number of output classes.\n",
        "\n",
        "    # Returns\n",
        "        An MLP model instance.\n",
        "    \"\"\"\n",
        "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
        "    model = models.Sequential()\n",
        "    #model.add(Dropout(rate = dropout_rate)\n",
        "    \n",
        "    for i in range(layers-1):\n",
        "      model.add(Dense(units = units, activation='relu'))\n",
        "      model.add(Dropout(rate = dropout_rate))\n",
        "\n",
        "    model.add(Dense(units=op_units, activation=op_activation))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB96fiqIl4h0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers = 2\n",
        "units = 16\n",
        "dropout_rate = 0.1\n",
        "#input_shape = 1\n",
        "num_classes = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCtGDpPIA0ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp_model = mlp_model(layers, units, dropout_rate, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWW8utx3HOs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp_model.compile(loss = 'sparse_categorical_crossentropy', \n",
        "              optimizer = 'adam', \n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tTcnJsKHker",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cd780dd4-b859-498a-a405-8095bdff5caa"
      },
      "source": [
        "NUM_EPOCHS = 500\n",
        "history = mlp_model.fit(train_vec, training_labels,\n",
        "                    validation_data = (val_vec, validation_labels),\n",
        "                    epochs = NUM_EPOCHS, \n",
        "                    callbacks = [checkpoint, callbacks])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 459 samples, validate on 183 samples\n",
            "Epoch 1/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1341 - accuracy: 0.4662 - val_loss: 15.6513 - val_accuracy: 0.2131\n",
            "Epoch 2/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1361 - accuracy: 0.4728 - val_loss: 15.6602 - val_accuracy: 0.2131\n",
            "Epoch 3/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1432 - accuracy: 0.4684 - val_loss: 15.6631 - val_accuracy: 0.2077\n",
            "Epoch 4/500\n",
            "459/459 [==============================] - 0s 87us/sample - loss: 1.1541 - accuracy: 0.4749 - val_loss: 15.9848 - val_accuracy: 0.2077\n",
            "Epoch 5/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1478 - accuracy: 0.4706 - val_loss: 15.8133 - val_accuracy: 0.2077\n",
            "Epoch 6/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1306 - accuracy: 0.4728 - val_loss: 15.8125 - val_accuracy: 0.2022\n",
            "Epoch 7/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1571 - accuracy: 0.4553 - val_loss: 15.7335 - val_accuracy: 0.1913\n",
            "Epoch 8/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1541 - accuracy: 0.4553 - val_loss: 15.6513 - val_accuracy: 0.2022\n",
            "Epoch 9/500\n",
            "459/459 [==============================] - 0s 67us/sample - loss: 1.1448 - accuracy: 0.4641 - val_loss: 15.5522 - val_accuracy: 0.2022\n",
            "Epoch 10/500\n",
            "459/459 [==============================] - 0s 67us/sample - loss: 1.1473 - accuracy: 0.4706 - val_loss: 15.5180 - val_accuracy: 0.2077\n",
            "Epoch 11/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1473 - accuracy: 0.4444 - val_loss: 15.2834 - val_accuracy: 0.2077\n",
            "Epoch 12/500\n",
            "459/459 [==============================] - 0s 67us/sample - loss: 1.1541 - accuracy: 0.4684 - val_loss: 15.5539 - val_accuracy: 0.2077\n",
            "Epoch 13/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1506 - accuracy: 0.4771 - val_loss: 15.3095 - val_accuracy: 0.2186\n",
            "Epoch 14/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1406 - accuracy: 0.4619 - val_loss: 15.4548 - val_accuracy: 0.2186\n",
            "Epoch 15/500\n",
            "459/459 [==============================] - 0s 65us/sample - loss: 1.1492 - accuracy: 0.4488 - val_loss: 15.6130 - val_accuracy: 0.2186\n",
            "Epoch 16/500\n",
            "459/459 [==============================] - 0s 67us/sample - loss: 1.1390 - accuracy: 0.4706 - val_loss: 15.5277 - val_accuracy: 0.2131\n",
            "Epoch 17/500\n",
            "459/459 [==============================] - 0s 64us/sample - loss: 1.1664 - accuracy: 0.4532 - val_loss: 15.5438 - val_accuracy: 0.2077\n",
            "Epoch 18/500\n",
            "459/459 [==============================] - 0s 66us/sample - loss: 1.1529 - accuracy: 0.4662 - val_loss: 15.7090 - val_accuracy: 0.2077\n",
            "Epoch 19/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1800 - accuracy: 0.4553 - val_loss: 15.6595 - val_accuracy: 0.2077\n",
            "Epoch 20/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1539 - accuracy: 0.4619 - val_loss: 15.4036 - val_accuracy: 0.2131\n",
            "Epoch 21/500\n",
            "459/459 [==============================] - 0s 87us/sample - loss: 1.1505 - accuracy: 0.4619 - val_loss: 15.4019 - val_accuracy: 0.2240\n",
            "Epoch 22/500\n",
            "459/459 [==============================] - 0s 90us/sample - loss: 1.1446 - accuracy: 0.4619 - val_loss: 15.5844 - val_accuracy: 0.2514\n",
            "Epoch 23/500\n",
            "459/459 [==============================] - 0s 95us/sample - loss: 1.1332 - accuracy: 0.4684 - val_loss: 15.6446 - val_accuracy: 0.2022\n",
            "Epoch 24/500\n",
            "459/459 [==============================] - 0s 102us/sample - loss: 1.1273 - accuracy: 0.4597 - val_loss: 15.3545 - val_accuracy: 0.2131\n",
            "Epoch 25/500\n",
            "459/459 [==============================] - 0s 90us/sample - loss: 1.1204 - accuracy: 0.4728 - val_loss: 15.2673 - val_accuracy: 0.2077\n",
            "Epoch 26/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1263 - accuracy: 0.4749 - val_loss: 15.5148 - val_accuracy: 0.1967\n",
            "Epoch 27/500\n",
            "459/459 [==============================] - 0s 65us/sample - loss: 1.1198 - accuracy: 0.4815 - val_loss: 16.2518 - val_accuracy: 0.1913\n",
            "Epoch 28/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1839 - accuracy: 0.4619 - val_loss: 16.4543 - val_accuracy: 0.1967\n",
            "Epoch 29/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1610 - accuracy: 0.4880 - val_loss: 15.9979 - val_accuracy: 0.2131\n",
            "Epoch 30/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.2120 - accuracy: 0.4553 - val_loss: 15.8541 - val_accuracy: 0.2022\n",
            "Epoch 31/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1880 - accuracy: 0.4619 - val_loss: 15.6929 - val_accuracy: 0.1913\n",
            "Epoch 32/500\n",
            "459/459 [==============================] - 0s 67us/sample - loss: 1.1616 - accuracy: 0.4641 - val_loss: 15.5189 - val_accuracy: 0.2131\n",
            "Epoch 33/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1481 - accuracy: 0.4444 - val_loss: 15.3390 - val_accuracy: 0.2131\n",
            "Epoch 34/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1528 - accuracy: 0.4597 - val_loss: 15.4708 - val_accuracy: 0.2131\n",
            "Epoch 35/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1637 - accuracy: 0.4771 - val_loss: 15.4919 - val_accuracy: 0.2131\n",
            "Epoch 36/500\n",
            "459/459 [==============================] - 0s 67us/sample - loss: 1.1867 - accuracy: 0.4553 - val_loss: 15.3989 - val_accuracy: 0.2295\n",
            "Epoch 37/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1332 - accuracy: 0.4706 - val_loss: 15.3321 - val_accuracy: 0.2404\n",
            "Epoch 38/500\n",
            "459/459 [==============================] - 0s 79us/sample - loss: 1.1577 - accuracy: 0.4641 - val_loss: 15.3652 - val_accuracy: 0.2240\n",
            "Epoch 39/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1787 - accuracy: 0.4466 - val_loss: 15.5185 - val_accuracy: 0.2459\n",
            "Epoch 40/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1430 - accuracy: 0.4575 - val_loss: 15.6167 - val_accuracy: 0.2350\n",
            "Epoch 41/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1345 - accuracy: 0.4684 - val_loss: 15.7040 - val_accuracy: 0.2186\n",
            "Epoch 42/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1553 - accuracy: 0.4662 - val_loss: 15.7540 - val_accuracy: 0.2131\n",
            "Epoch 43/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1404 - accuracy: 0.4684 - val_loss: 16.9619 - val_accuracy: 0.2350\n",
            "Epoch 44/500\n",
            "459/459 [==============================] - 0s 82us/sample - loss: 1.1848 - accuracy: 0.4510 - val_loss: 16.7373 - val_accuracy: 0.2514\n",
            "Epoch 45/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1360 - accuracy: 0.4641 - val_loss: 16.1655 - val_accuracy: 0.2404\n",
            "Epoch 46/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1706 - accuracy: 0.4553 - val_loss: 15.8970 - val_accuracy: 0.2404\n",
            "Epoch 47/500\n",
            "459/459 [==============================] - 0s 80us/sample - loss: 1.1382 - accuracy: 0.4641 - val_loss: 15.9096 - val_accuracy: 0.2186\n",
            "Epoch 48/500\n",
            "459/459 [==============================] - 0s 83us/sample - loss: 1.1659 - accuracy: 0.4619 - val_loss: 15.6422 - val_accuracy: 0.2350\n",
            "Epoch 49/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1815 - accuracy: 0.4466 - val_loss: 15.6291 - val_accuracy: 0.2295\n",
            "Epoch 50/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1538 - accuracy: 0.4641 - val_loss: 15.6209 - val_accuracy: 0.2240\n",
            "Epoch 51/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1771 - accuracy: 0.4553 - val_loss: 15.5368 - val_accuracy: 0.2295\n",
            "Epoch 52/500\n",
            "459/459 [==============================] - 0s 99us/sample - loss: 1.1565 - accuracy: 0.4728 - val_loss: 15.3979 - val_accuracy: 0.2186\n",
            "Epoch 53/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1175 - accuracy: 0.4815 - val_loss: 15.5556 - val_accuracy: 0.2186\n",
            "Epoch 54/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1205 - accuracy: 0.4684 - val_loss: 15.8113 - val_accuracy: 0.2240\n",
            "Epoch 55/500\n",
            "459/459 [==============================] - 0s 79us/sample - loss: 1.1743 - accuracy: 0.4488 - val_loss: 16.1195 - val_accuracy: 0.2459\n",
            "Epoch 56/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1492 - accuracy: 0.4662 - val_loss: 16.0989 - val_accuracy: 0.2404\n",
            "Epoch 57/500\n",
            "459/459 [==============================] - 0s 84us/sample - loss: 1.1555 - accuracy: 0.4619 - val_loss: 16.0544 - val_accuracy: 0.2404\n",
            "Epoch 58/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1268 - accuracy: 0.4749 - val_loss: 15.9239 - val_accuracy: 0.2240\n",
            "Epoch 59/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1603 - accuracy: 0.4510 - val_loss: 15.5582 - val_accuracy: 0.2295\n",
            "Epoch 60/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1850 - accuracy: 0.4532 - val_loss: 15.4737 - val_accuracy: 0.2404\n",
            "Epoch 61/500\n",
            "459/459 [==============================] - 0s 87us/sample - loss: 1.1565 - accuracy: 0.4662 - val_loss: 15.5344 - val_accuracy: 0.2404\n",
            "Epoch 62/500\n",
            "459/459 [==============================] - 0s 90us/sample - loss: 1.1570 - accuracy: 0.4902 - val_loss: 15.5810 - val_accuracy: 0.2404\n",
            "Epoch 63/500\n",
            "459/459 [==============================] - 0s 82us/sample - loss: 1.1809 - accuracy: 0.4510 - val_loss: 15.4470 - val_accuracy: 0.2459\n",
            "Epoch 64/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1632 - accuracy: 0.4662 - val_loss: 15.4135 - val_accuracy: 0.2459\n",
            "Epoch 65/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1690 - accuracy: 0.4641 - val_loss: 15.1587 - val_accuracy: 0.2404\n",
            "Epoch 66/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1243 - accuracy: 0.4706 - val_loss: 15.2543 - val_accuracy: 0.2295\n",
            "Epoch 67/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1783 - accuracy: 0.4532 - val_loss: 15.7136 - val_accuracy: 0.2295\n",
            "Epoch 68/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1434 - accuracy: 0.4510 - val_loss: 15.7866 - val_accuracy: 0.2295\n",
            "Epoch 69/500\n",
            "459/459 [==============================] - 0s 80us/sample - loss: 1.1349 - accuracy: 0.4619 - val_loss: 15.6802 - val_accuracy: 0.2240\n",
            "Epoch 70/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1599 - accuracy: 0.4619 - val_loss: 15.5427 - val_accuracy: 0.2240\n",
            "Epoch 71/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1594 - accuracy: 0.4662 - val_loss: 15.6056 - val_accuracy: 0.2295\n",
            "Epoch 72/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1471 - accuracy: 0.4793 - val_loss: 15.7350 - val_accuracy: 0.2350\n",
            "Epoch 73/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1351 - accuracy: 0.4728 - val_loss: 15.6087 - val_accuracy: 0.2514\n",
            "Epoch 74/500\n",
            "459/459 [==============================] - 0s 88us/sample - loss: 1.1550 - accuracy: 0.4728 - val_loss: 15.4141 - val_accuracy: 0.2350\n",
            "Epoch 75/500\n",
            "459/459 [==============================] - 0s 95us/sample - loss: 1.1479 - accuracy: 0.4684 - val_loss: 15.5997 - val_accuracy: 0.2350\n",
            "Epoch 76/500\n",
            "459/459 [==============================] - 0s 86us/sample - loss: 1.1340 - accuracy: 0.4815 - val_loss: 15.7066 - val_accuracy: 0.2295\n",
            "Epoch 77/500\n",
            "459/459 [==============================] - 0s 83us/sample - loss: 1.1942 - accuracy: 0.4619 - val_loss: 15.8251 - val_accuracy: 0.2131\n",
            "Epoch 78/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1435 - accuracy: 0.4619 - val_loss: 15.9742 - val_accuracy: 0.2295\n",
            "Epoch 79/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1962 - accuracy: 0.4575 - val_loss: 16.0947 - val_accuracy: 0.2350\n",
            "Epoch 80/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1730 - accuracy: 0.4553 - val_loss: 15.8972 - val_accuracy: 0.2404\n",
            "Epoch 81/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1561 - accuracy: 0.4575 - val_loss: 15.9089 - val_accuracy: 0.2350\n",
            "Epoch 82/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1286 - accuracy: 0.4793 - val_loss: 15.8056 - val_accuracy: 0.2240\n",
            "Epoch 83/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1278 - accuracy: 0.4793 - val_loss: 15.7659 - val_accuracy: 0.2350\n",
            "Epoch 84/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1275 - accuracy: 0.4837 - val_loss: 15.7842 - val_accuracy: 0.2350\n",
            "Epoch 85/500\n",
            "459/459 [==============================] - 0s 81us/sample - loss: 1.1283 - accuracy: 0.4728 - val_loss: 15.7830 - val_accuracy: 0.2350\n",
            "Epoch 86/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1318 - accuracy: 0.4641 - val_loss: 15.6848 - val_accuracy: 0.2404\n",
            "Epoch 87/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1134 - accuracy: 0.4749 - val_loss: 15.6104 - val_accuracy: 0.2459\n",
            "Epoch 88/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1682 - accuracy: 0.4684 - val_loss: 15.6714 - val_accuracy: 0.2404\n",
            "Epoch 89/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1200 - accuracy: 0.4815 - val_loss: 15.7043 - val_accuracy: 0.2350\n",
            "Epoch 90/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1669 - accuracy: 0.4662 - val_loss: 15.6496 - val_accuracy: 0.2186\n",
            "Epoch 91/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1283 - accuracy: 0.4728 - val_loss: 15.6776 - val_accuracy: 0.2350\n",
            "Epoch 92/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1114 - accuracy: 0.4728 - val_loss: 16.2327 - val_accuracy: 0.2350\n",
            "Epoch 93/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1413 - accuracy: 0.4662 - val_loss: 16.3373 - val_accuracy: 0.2404\n",
            "Epoch 94/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1333 - accuracy: 0.4837 - val_loss: 16.1863 - val_accuracy: 0.2295\n",
            "Epoch 95/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1344 - accuracy: 0.4662 - val_loss: 16.3786 - val_accuracy: 0.2404\n",
            "Epoch 96/500\n",
            "459/459 [==============================] - 0s 82us/sample - loss: 1.1371 - accuracy: 0.4837 - val_loss: 16.4569 - val_accuracy: 0.2350\n",
            "Epoch 97/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.2149 - accuracy: 0.4444 - val_loss: 16.4636 - val_accuracy: 0.2350\n",
            "Epoch 98/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1666 - accuracy: 0.4662 - val_loss: 16.4761 - val_accuracy: 0.2459\n",
            "Epoch 99/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1887 - accuracy: 0.4488 - val_loss: 16.4683 - val_accuracy: 0.2459\n",
            "Epoch 100/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1488 - accuracy: 0.4641 - val_loss: 16.5004 - val_accuracy: 0.2240\n",
            "Epoch 101/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1312 - accuracy: 0.4728 - val_loss: 16.4960 - val_accuracy: 0.2295\n",
            "Epoch 102/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1358 - accuracy: 0.4706 - val_loss: 16.5059 - val_accuracy: 0.2350\n",
            "Epoch 103/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1598 - accuracy: 0.4662 - val_loss: 16.6503 - val_accuracy: 0.2350\n",
            "Epoch 104/500\n",
            "459/459 [==============================] - 0s 80us/sample - loss: 1.1531 - accuracy: 0.4553 - val_loss: 16.2096 - val_accuracy: 0.2459\n",
            "Epoch 105/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1408 - accuracy: 0.4728 - val_loss: 16.2598 - val_accuracy: 0.2295\n",
            "Epoch 106/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1788 - accuracy: 0.4662 - val_loss: 16.1256 - val_accuracy: 0.2459\n",
            "Epoch 107/500\n",
            "459/459 [==============================] - 0s 79us/sample - loss: 1.1614 - accuracy: 0.4815 - val_loss: 16.4753 - val_accuracy: 0.2404\n",
            "Epoch 108/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1429 - accuracy: 0.4880 - val_loss: 16.4578 - val_accuracy: 0.2623\n",
            "Epoch 109/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1389 - accuracy: 0.4880 - val_loss: 15.9913 - val_accuracy: 0.2459\n",
            "Epoch 110/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1854 - accuracy: 0.4553 - val_loss: 15.6627 - val_accuracy: 0.2514\n",
            "Epoch 111/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1850 - accuracy: 0.4510 - val_loss: 15.8056 - val_accuracy: 0.2295\n",
            "Epoch 112/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1313 - accuracy: 0.4771 - val_loss: 16.7176 - val_accuracy: 0.2295\n",
            "Epoch 113/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.2035 - accuracy: 0.4684 - val_loss: 16.4388 - val_accuracy: 0.2350\n",
            "Epoch 114/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1494 - accuracy: 0.4619 - val_loss: 16.1374 - val_accuracy: 0.2350\n",
            "Epoch 115/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1983 - accuracy: 0.4466 - val_loss: 16.0263 - val_accuracy: 0.2459\n",
            "Epoch 116/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1474 - accuracy: 0.4706 - val_loss: 15.7954 - val_accuracy: 0.2295\n",
            "Epoch 117/500\n",
            "459/459 [==============================] - 0s 86us/sample - loss: 1.1384 - accuracy: 0.4706 - val_loss: 15.7284 - val_accuracy: 0.2459\n",
            "Epoch 118/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1978 - accuracy: 0.4684 - val_loss: 15.6695 - val_accuracy: 0.2186\n",
            "Epoch 119/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1940 - accuracy: 0.4510 - val_loss: 15.7758 - val_accuracy: 0.2240\n",
            "Epoch 120/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1601 - accuracy: 0.4662 - val_loss: 15.8889 - val_accuracy: 0.2295\n",
            "Epoch 121/500\n",
            "459/459 [==============================] - 0s 84us/sample - loss: 1.1805 - accuracy: 0.4466 - val_loss: 15.8309 - val_accuracy: 0.2568\n",
            "Epoch 122/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1616 - accuracy: 0.4488 - val_loss: 15.8007 - val_accuracy: 0.2568\n",
            "Epoch 123/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1501 - accuracy: 0.4423 - val_loss: 15.6447 - val_accuracy: 0.2514\n",
            "Epoch 124/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1798 - accuracy: 0.4532 - val_loss: 15.4067 - val_accuracy: 0.2350\n",
            "Epoch 125/500\n",
            "459/459 [==============================] - 0s 83us/sample - loss: 1.1596 - accuracy: 0.4597 - val_loss: 15.3785 - val_accuracy: 0.2240\n",
            "Epoch 126/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1473 - accuracy: 0.4597 - val_loss: 15.4383 - val_accuracy: 0.2295\n",
            "Epoch 127/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1661 - accuracy: 0.4749 - val_loss: 15.4292 - val_accuracy: 0.2404\n",
            "Epoch 128/500\n",
            "459/459 [==============================] - 0s 67us/sample - loss: 1.1638 - accuracy: 0.4510 - val_loss: 15.3910 - val_accuracy: 0.2350\n",
            "Epoch 129/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1544 - accuracy: 0.4641 - val_loss: 15.3337 - val_accuracy: 0.2295\n",
            "Epoch 130/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1748 - accuracy: 0.4662 - val_loss: 15.2252 - val_accuracy: 0.2295\n",
            "Epoch 131/500\n",
            "459/459 [==============================] - 0s 66us/sample - loss: 1.1723 - accuracy: 0.4749 - val_loss: 15.2282 - val_accuracy: 0.2350\n",
            "Epoch 132/500\n",
            "459/459 [==============================] - 0s 64us/sample - loss: 1.1682 - accuracy: 0.4684 - val_loss: 15.2621 - val_accuracy: 0.2350\n",
            "Epoch 133/500\n",
            "459/459 [==============================] - 0s 79us/sample - loss: 1.1505 - accuracy: 0.4619 - val_loss: 15.3490 - val_accuracy: 0.2295\n",
            "Epoch 134/500\n",
            "459/459 [==============================] - 0s 81us/sample - loss: 1.1641 - accuracy: 0.4575 - val_loss: 15.4565 - val_accuracy: 0.2459\n",
            "Epoch 135/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1481 - accuracy: 0.4662 - val_loss: 15.4654 - val_accuracy: 0.2350\n",
            "Epoch 136/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1767 - accuracy: 0.4423 - val_loss: 15.3532 - val_accuracy: 0.2404\n",
            "Epoch 137/500\n",
            "459/459 [==============================] - 0s 66us/sample - loss: 1.1188 - accuracy: 0.4793 - val_loss: 15.3277 - val_accuracy: 0.2350\n",
            "Epoch 138/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1160 - accuracy: 0.4771 - val_loss: 15.3740 - val_accuracy: 0.2350\n",
            "Epoch 139/500\n",
            "459/459 [==============================] - 0s 65us/sample - loss: 1.1284 - accuracy: 0.4684 - val_loss: 15.4768 - val_accuracy: 0.2404\n",
            "Epoch 140/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1434 - accuracy: 0.4706 - val_loss: 15.3218 - val_accuracy: 0.2240\n",
            "Epoch 141/500\n",
            "459/459 [==============================] - 0s 65us/sample - loss: 1.2180 - accuracy: 0.4444 - val_loss: 15.2343 - val_accuracy: 0.2459\n",
            "Epoch 142/500\n",
            "459/459 [==============================] - 0s 66us/sample - loss: 1.1405 - accuracy: 0.4619 - val_loss: 15.2252 - val_accuracy: 0.2514\n",
            "Epoch 143/500\n",
            "459/459 [==============================] - 0s 87us/sample - loss: 1.1609 - accuracy: 0.4510 - val_loss: 15.2095 - val_accuracy: 0.2459\n",
            "Epoch 144/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.3081 - accuracy: 0.4532 - val_loss: 15.2632 - val_accuracy: 0.2350\n",
            "Epoch 145/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1312 - accuracy: 0.4619 - val_loss: 15.3464 - val_accuracy: 0.2404\n",
            "Epoch 146/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1527 - accuracy: 0.4510 - val_loss: 15.6447 - val_accuracy: 0.2459\n",
            "Epoch 147/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1672 - accuracy: 0.4597 - val_loss: 15.6342 - val_accuracy: 0.2404\n",
            "Epoch 148/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1517 - accuracy: 0.4532 - val_loss: 15.6643 - val_accuracy: 0.2295\n",
            "Epoch 149/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1608 - accuracy: 0.4619 - val_loss: 15.6490 - val_accuracy: 0.2240\n",
            "Epoch 150/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1624 - accuracy: 0.4684 - val_loss: 15.6338 - val_accuracy: 0.2240\n",
            "Epoch 151/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1463 - accuracy: 0.4619 - val_loss: 15.6770 - val_accuracy: 0.2240\n",
            "Epoch 152/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1280 - accuracy: 0.4771 - val_loss: 15.6141 - val_accuracy: 0.2240\n",
            "Epoch 153/500\n",
            "459/459 [==============================] - 0s 64us/sample - loss: 1.1412 - accuracy: 0.4619 - val_loss: 15.6017 - val_accuracy: 0.2240\n",
            "Epoch 154/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1537 - accuracy: 0.4641 - val_loss: 15.8345 - val_accuracy: 0.2240\n",
            "Epoch 155/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1732 - accuracy: 0.4619 - val_loss: 15.9739 - val_accuracy: 0.2350\n",
            "Epoch 156/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1436 - accuracy: 0.4706 - val_loss: 15.8611 - val_accuracy: 0.2240\n",
            "Epoch 157/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1700 - accuracy: 0.4532 - val_loss: 15.7200 - val_accuracy: 0.2350\n",
            "Epoch 158/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1826 - accuracy: 0.4575 - val_loss: 15.7385 - val_accuracy: 0.2295\n",
            "Epoch 159/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1781 - accuracy: 0.4488 - val_loss: 15.7356 - val_accuracy: 0.2186\n",
            "Epoch 160/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1496 - accuracy: 0.4510 - val_loss: 15.8539 - val_accuracy: 0.2131\n",
            "Epoch 161/500\n",
            "459/459 [==============================] - 0s 67us/sample - loss: 1.1605 - accuracy: 0.4553 - val_loss: 15.9408 - val_accuracy: 0.2186\n",
            "Epoch 162/500\n",
            "459/459 [==============================] - 0s 94us/sample - loss: 1.1395 - accuracy: 0.4749 - val_loss: 15.9570 - val_accuracy: 0.2131\n",
            "Epoch 163/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1559 - accuracy: 0.4553 - val_loss: 15.9784 - val_accuracy: 0.2077\n",
            "Epoch 164/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1412 - accuracy: 0.4749 - val_loss: 16.0142 - val_accuracy: 0.2131\n",
            "Epoch 165/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1517 - accuracy: 0.4619 - val_loss: 15.9232 - val_accuracy: 0.2131\n",
            "Epoch 166/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1536 - accuracy: 0.4662 - val_loss: 15.8780 - val_accuracy: 0.2131\n",
            "Epoch 167/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1248 - accuracy: 0.4771 - val_loss: 15.8938 - val_accuracy: 0.2240\n",
            "Epoch 168/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1749 - accuracy: 0.4728 - val_loss: 15.9175 - val_accuracy: 0.2131\n",
            "Epoch 169/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1405 - accuracy: 0.4793 - val_loss: 15.9078 - val_accuracy: 0.2240\n",
            "Epoch 170/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1364 - accuracy: 0.4575 - val_loss: 15.8968 - val_accuracy: 0.2295\n",
            "Epoch 171/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1398 - accuracy: 0.4749 - val_loss: 15.8779 - val_accuracy: 0.2131\n",
            "Epoch 172/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1289 - accuracy: 0.4706 - val_loss: 15.6958 - val_accuracy: 0.2186\n",
            "Epoch 173/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1441 - accuracy: 0.4553 - val_loss: 15.7543 - val_accuracy: 0.2131\n",
            "Epoch 174/500\n",
            "459/459 [==============================] - 0s 86us/sample - loss: 1.1459 - accuracy: 0.4575 - val_loss: 15.6784 - val_accuracy: 0.2022\n",
            "Epoch 175/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1372 - accuracy: 0.4619 - val_loss: 15.5762 - val_accuracy: 0.2077\n",
            "Epoch 176/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1503 - accuracy: 0.4793 - val_loss: 15.3781 - val_accuracy: 0.1967\n",
            "Epoch 177/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1185 - accuracy: 0.4880 - val_loss: 15.4024 - val_accuracy: 0.1967\n",
            "Epoch 178/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1446 - accuracy: 0.4597 - val_loss: 15.5917 - val_accuracy: 0.1967\n",
            "Epoch 179/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1265 - accuracy: 0.4771 - val_loss: 15.6310 - val_accuracy: 0.2131\n",
            "Epoch 180/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1633 - accuracy: 0.4684 - val_loss: 15.6579 - val_accuracy: 0.2077\n",
            "Epoch 181/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1367 - accuracy: 0.4641 - val_loss: 15.6585 - val_accuracy: 0.2022\n",
            "Epoch 182/500\n",
            "459/459 [==============================] - 0s 84us/sample - loss: 1.1465 - accuracy: 0.4597 - val_loss: 15.5691 - val_accuracy: 0.2022\n",
            "Epoch 183/500\n",
            "459/459 [==============================] - 0s 81us/sample - loss: 1.1853 - accuracy: 0.4553 - val_loss: 16.4828 - val_accuracy: 0.2350\n",
            "Epoch 184/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1745 - accuracy: 0.4728 - val_loss: 16.2511 - val_accuracy: 0.2186\n",
            "Epoch 185/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1444 - accuracy: 0.4706 - val_loss: 15.7989 - val_accuracy: 0.2077\n",
            "Epoch 186/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1691 - accuracy: 0.4619 - val_loss: 15.1549 - val_accuracy: 0.2295\n",
            "Epoch 187/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1297 - accuracy: 0.4684 - val_loss: 14.9880 - val_accuracy: 0.2350\n",
            "Epoch 188/500\n",
            "459/459 [==============================] - 0s 80us/sample - loss: 1.1598 - accuracy: 0.4553 - val_loss: 14.9832 - val_accuracy: 0.2240\n",
            "Epoch 189/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1496 - accuracy: 0.4619 - val_loss: 14.9735 - val_accuracy: 0.2240\n",
            "Epoch 190/500\n",
            "459/459 [==============================] - 0s 88us/sample - loss: 1.1629 - accuracy: 0.4575 - val_loss: 14.9950 - val_accuracy: 0.2186\n",
            "Epoch 191/500\n",
            "459/459 [==============================] - 0s 84us/sample - loss: 1.1817 - accuracy: 0.4728 - val_loss: 15.4607 - val_accuracy: 0.2131\n",
            "Epoch 192/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1546 - accuracy: 0.4684 - val_loss: 16.3919 - val_accuracy: 0.2022\n",
            "Epoch 193/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1358 - accuracy: 0.4597 - val_loss: 16.2010 - val_accuracy: 0.2077\n",
            "Epoch 194/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1310 - accuracy: 0.4749 - val_loss: 16.0416 - val_accuracy: 0.2022\n",
            "Epoch 195/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1520 - accuracy: 0.4749 - val_loss: 15.7933 - val_accuracy: 0.2131\n",
            "Epoch 196/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1422 - accuracy: 0.4706 - val_loss: 15.5071 - val_accuracy: 0.2240\n",
            "Epoch 197/500\n",
            "459/459 [==============================] - 0s 79us/sample - loss: 1.1422 - accuracy: 0.4706 - val_loss: 15.3266 - val_accuracy: 0.2131\n",
            "Epoch 198/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1436 - accuracy: 0.4641 - val_loss: 15.1688 - val_accuracy: 0.2077\n",
            "Epoch 199/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.2119 - accuracy: 0.4488 - val_loss: 15.3750 - val_accuracy: 0.2295\n",
            "Epoch 200/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1562 - accuracy: 0.4510 - val_loss: 15.6086 - val_accuracy: 0.2240\n",
            "Epoch 201/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1506 - accuracy: 0.4532 - val_loss: 15.5254 - val_accuracy: 0.2240\n",
            "Epoch 202/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1563 - accuracy: 0.4684 - val_loss: 15.4205 - val_accuracy: 0.2350\n",
            "Epoch 203/500\n",
            "459/459 [==============================] - 0s 65us/sample - loss: 1.1577 - accuracy: 0.4641 - val_loss: 15.4009 - val_accuracy: 0.2186\n",
            "Epoch 204/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1260 - accuracy: 0.4815 - val_loss: 15.5946 - val_accuracy: 0.2240\n",
            "Epoch 205/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1315 - accuracy: 0.4684 - val_loss: 15.5883 - val_accuracy: 0.2186\n",
            "Epoch 206/500\n",
            "459/459 [==============================] - 0s 64us/sample - loss: 1.1603 - accuracy: 0.4641 - val_loss: 15.6419 - val_accuracy: 0.2186\n",
            "Epoch 207/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1373 - accuracy: 0.4662 - val_loss: 15.6161 - val_accuracy: 0.2240\n",
            "Epoch 208/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1678 - accuracy: 0.4597 - val_loss: 16.1852 - val_accuracy: 0.2022\n",
            "Epoch 209/500\n",
            "459/459 [==============================] - 0s 67us/sample - loss: 1.1299 - accuracy: 0.4706 - val_loss: 16.3125 - val_accuracy: 0.2022\n",
            "Epoch 210/500\n",
            "459/459 [==============================] - 0s 65us/sample - loss: 1.1601 - accuracy: 0.4641 - val_loss: 16.4127 - val_accuracy: 0.2022\n",
            "Epoch 211/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1476 - accuracy: 0.4684 - val_loss: 16.2075 - val_accuracy: 0.2022\n",
            "Epoch 212/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1417 - accuracy: 0.4619 - val_loss: 16.0464 - val_accuracy: 0.2077\n",
            "Epoch 213/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1949 - accuracy: 0.4466 - val_loss: 16.0108 - val_accuracy: 0.2131\n",
            "Epoch 214/500\n",
            "459/459 [==============================] - 0s 87us/sample - loss: 1.1542 - accuracy: 0.4662 - val_loss: 15.8862 - val_accuracy: 0.2186\n",
            "Epoch 215/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1259 - accuracy: 0.4771 - val_loss: 15.7553 - val_accuracy: 0.2077\n",
            "Epoch 216/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1391 - accuracy: 0.4815 - val_loss: 15.7065 - val_accuracy: 0.2022\n",
            "Epoch 217/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1506 - accuracy: 0.4771 - val_loss: 15.5705 - val_accuracy: 0.2022\n",
            "Epoch 218/500\n",
            "459/459 [==============================] - 0s 65us/sample - loss: 1.1374 - accuracy: 0.4641 - val_loss: 15.4476 - val_accuracy: 0.2131\n",
            "Epoch 219/500\n",
            "459/459 [==============================] - 0s 80us/sample - loss: 1.1459 - accuracy: 0.4641 - val_loss: 15.3786 - val_accuracy: 0.2186\n",
            "Epoch 220/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1598 - accuracy: 0.4641 - val_loss: 15.6237 - val_accuracy: 0.2186\n",
            "Epoch 221/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1499 - accuracy: 0.4641 - val_loss: 16.0336 - val_accuracy: 0.2022\n",
            "Epoch 222/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1699 - accuracy: 0.4619 - val_loss: 15.9519 - val_accuracy: 0.2022\n",
            "Epoch 223/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1550 - accuracy: 0.4641 - val_loss: 15.9306 - val_accuracy: 0.2022\n",
            "Epoch 224/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1441 - accuracy: 0.4553 - val_loss: 15.8647 - val_accuracy: 0.2240\n",
            "Epoch 225/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1628 - accuracy: 0.4466 - val_loss: 15.8341 - val_accuracy: 0.2131\n",
            "Epoch 226/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1449 - accuracy: 0.4597 - val_loss: 15.9040 - val_accuracy: 0.2186\n",
            "Epoch 227/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1658 - accuracy: 0.4444 - val_loss: 15.9207 - val_accuracy: 0.2186\n",
            "Epoch 228/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1369 - accuracy: 0.4575 - val_loss: 15.9238 - val_accuracy: 0.2240\n",
            "Epoch 229/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1310 - accuracy: 0.4662 - val_loss: 15.8262 - val_accuracy: 0.2186\n",
            "Epoch 230/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1361 - accuracy: 0.4662 - val_loss: 15.8438 - val_accuracy: 0.2186\n",
            "Epoch 231/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1739 - accuracy: 0.4575 - val_loss: 15.7935 - val_accuracy: 0.2404\n",
            "Epoch 232/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1330 - accuracy: 0.4728 - val_loss: 15.8675 - val_accuracy: 0.2295\n",
            "Epoch 233/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1321 - accuracy: 0.4532 - val_loss: 15.9738 - val_accuracy: 0.2077\n",
            "Epoch 234/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1381 - accuracy: 0.4619 - val_loss: 16.0154 - val_accuracy: 0.2131\n",
            "Epoch 235/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1420 - accuracy: 0.4749 - val_loss: 15.9821 - val_accuracy: 0.2022\n",
            "Epoch 236/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1172 - accuracy: 0.4706 - val_loss: 15.9097 - val_accuracy: 0.2240\n",
            "Epoch 237/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1442 - accuracy: 0.4706 - val_loss: 15.8180 - val_accuracy: 0.2240\n",
            "Epoch 238/500\n",
            "459/459 [==============================] - 0s 66us/sample - loss: 1.1661 - accuracy: 0.4662 - val_loss: 15.7856 - val_accuracy: 0.2131\n",
            "Epoch 239/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1469 - accuracy: 0.4771 - val_loss: 15.7952 - val_accuracy: 0.2186\n",
            "Epoch 240/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1765 - accuracy: 0.4510 - val_loss: 15.8286 - val_accuracy: 0.2240\n",
            "Epoch 241/500\n",
            "459/459 [==============================] - 0s 66us/sample - loss: 1.1307 - accuracy: 0.4575 - val_loss: 15.8973 - val_accuracy: 0.2077\n",
            "Epoch 242/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.2105 - accuracy: 0.4314 - val_loss: 15.8669 - val_accuracy: 0.2131\n",
            "Epoch 243/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1450 - accuracy: 0.4553 - val_loss: 15.9643 - val_accuracy: 0.2240\n",
            "Epoch 244/500\n",
            "459/459 [==============================] - 0s 79us/sample - loss: 1.1773 - accuracy: 0.4597 - val_loss: 15.8464 - val_accuracy: 0.2131\n",
            "Epoch 245/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.2045 - accuracy: 0.4336 - val_loss: 15.8592 - val_accuracy: 0.2240\n",
            "Epoch 246/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1469 - accuracy: 0.4641 - val_loss: 15.6869 - val_accuracy: 0.2240\n",
            "Epoch 247/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1971 - accuracy: 0.4466 - val_loss: 15.7917 - val_accuracy: 0.2186\n",
            "Epoch 248/500\n",
            "459/459 [==============================] - 0s 92us/sample - loss: 1.1326 - accuracy: 0.4532 - val_loss: 15.8591 - val_accuracy: 0.2240\n",
            "Epoch 249/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1468 - accuracy: 0.4466 - val_loss: 15.4355 - val_accuracy: 0.2240\n",
            "Epoch 250/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1688 - accuracy: 0.4662 - val_loss: 15.5032 - val_accuracy: 0.2295\n",
            "Epoch 251/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.2011 - accuracy: 0.4597 - val_loss: 15.5048 - val_accuracy: 0.2295\n",
            "Epoch 252/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1436 - accuracy: 0.4641 - val_loss: 15.4686 - val_accuracy: 0.2240\n",
            "Epoch 253/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1452 - accuracy: 0.4553 - val_loss: 15.5102 - val_accuracy: 0.2186\n",
            "Epoch 254/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1885 - accuracy: 0.4444 - val_loss: 15.6152 - val_accuracy: 0.2404\n",
            "Epoch 255/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1484 - accuracy: 0.4641 - val_loss: 15.6580 - val_accuracy: 0.2350\n",
            "Epoch 256/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1343 - accuracy: 0.4575 - val_loss: 15.7076 - val_accuracy: 0.2350\n",
            "Epoch 257/500\n",
            "459/459 [==============================] - 0s 63us/sample - loss: 1.1515 - accuracy: 0.4706 - val_loss: 15.7474 - val_accuracy: 0.2131\n",
            "Epoch 258/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1920 - accuracy: 0.4597 - val_loss: 15.9095 - val_accuracy: 0.2240\n",
            "Epoch 259/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1322 - accuracy: 0.4728 - val_loss: 15.9647 - val_accuracy: 0.2295\n",
            "Epoch 260/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1396 - accuracy: 0.4532 - val_loss: 16.0010 - val_accuracy: 0.2240\n",
            "Epoch 261/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1658 - accuracy: 0.4466 - val_loss: 15.8797 - val_accuracy: 0.2295\n",
            "Epoch 262/500\n",
            "459/459 [==============================] - 0s 65us/sample - loss: 1.1659 - accuracy: 0.4553 - val_loss: 15.7846 - val_accuracy: 0.2295\n",
            "Epoch 263/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1610 - accuracy: 0.4488 - val_loss: 15.7389 - val_accuracy: 0.2350\n",
            "Epoch 264/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1066 - accuracy: 0.4706 - val_loss: 15.7626 - val_accuracy: 0.2295\n",
            "Epoch 265/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1278 - accuracy: 0.4706 - val_loss: 15.8525 - val_accuracy: 0.2240\n",
            "Epoch 266/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1726 - accuracy: 0.4401 - val_loss: 15.9965 - val_accuracy: 0.2240\n",
            "Epoch 267/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1300 - accuracy: 0.4684 - val_loss: 15.9957 - val_accuracy: 0.2404\n",
            "Epoch 268/500\n",
            "459/459 [==============================] - 0s 67us/sample - loss: 1.1764 - accuracy: 0.4641 - val_loss: 15.8238 - val_accuracy: 0.2459\n",
            "Epoch 269/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1245 - accuracy: 0.4706 - val_loss: 15.4988 - val_accuracy: 0.2350\n",
            "Epoch 270/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1496 - accuracy: 0.4858 - val_loss: 15.5782 - val_accuracy: 0.2295\n",
            "Epoch 271/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1450 - accuracy: 0.4641 - val_loss: 15.5099 - val_accuracy: 0.2295\n",
            "Epoch 272/500\n",
            "459/459 [==============================] - 0s 84us/sample - loss: 1.2170 - accuracy: 0.4357 - val_loss: 15.6692 - val_accuracy: 0.2459\n",
            "Epoch 273/500\n",
            "459/459 [==============================] - 0s 79us/sample - loss: 1.1580 - accuracy: 0.4662 - val_loss: 15.8495 - val_accuracy: 0.2404\n",
            "Epoch 274/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1704 - accuracy: 0.4466 - val_loss: 15.3740 - val_accuracy: 0.2295\n",
            "Epoch 275/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1751 - accuracy: 0.4553 - val_loss: 15.5139 - val_accuracy: 0.1967\n",
            "Epoch 276/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1540 - accuracy: 0.4488 - val_loss: 15.5761 - val_accuracy: 0.2186\n",
            "Epoch 277/500\n",
            "459/459 [==============================] - 0s 83us/sample - loss: 1.1278 - accuracy: 0.4728 - val_loss: 15.5566 - val_accuracy: 0.2186\n",
            "Epoch 278/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1149 - accuracy: 0.4837 - val_loss: 15.5625 - val_accuracy: 0.2186\n",
            "Epoch 279/500\n",
            "459/459 [==============================] - 0s 79us/sample - loss: 1.1549 - accuracy: 0.4662 - val_loss: 15.5950 - val_accuracy: 0.2186\n",
            "Epoch 280/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1830 - accuracy: 0.4553 - val_loss: 15.6619 - val_accuracy: 0.2131\n",
            "Epoch 281/500\n",
            "459/459 [==============================] - 0s 65us/sample - loss: 1.1471 - accuracy: 0.4684 - val_loss: 15.6890 - val_accuracy: 0.2240\n",
            "Epoch 282/500\n",
            "459/459 [==============================] - 0s 66us/sample - loss: 1.1364 - accuracy: 0.4771 - val_loss: 15.7420 - val_accuracy: 0.2186\n",
            "Epoch 283/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1323 - accuracy: 0.4728 - val_loss: 15.9477 - val_accuracy: 0.2240\n",
            "Epoch 284/500\n",
            "459/459 [==============================] - 0s 65us/sample - loss: 1.1282 - accuracy: 0.4597 - val_loss: 15.9418 - val_accuracy: 0.2186\n",
            "Epoch 285/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1130 - accuracy: 0.4771 - val_loss: 15.8813 - val_accuracy: 0.2131\n",
            "Epoch 286/500\n",
            "459/459 [==============================] - 0s 66us/sample - loss: 1.1483 - accuracy: 0.4815 - val_loss: 15.7753 - val_accuracy: 0.2186\n",
            "Epoch 287/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1676 - accuracy: 0.4575 - val_loss: 15.8140 - val_accuracy: 0.2240\n",
            "Epoch 288/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1381 - accuracy: 0.4706 - val_loss: 16.0707 - val_accuracy: 0.2350\n",
            "Epoch 289/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1736 - accuracy: 0.4706 - val_loss: 16.1270 - val_accuracy: 0.2240\n",
            "Epoch 290/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1281 - accuracy: 0.4706 - val_loss: 16.3096 - val_accuracy: 0.2240\n",
            "Epoch 291/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1423 - accuracy: 0.4662 - val_loss: 16.4538 - val_accuracy: 0.2240\n",
            "Epoch 292/500\n",
            "459/459 [==============================] - 0s 67us/sample - loss: 1.1318 - accuracy: 0.4728 - val_loss: 16.3042 - val_accuracy: 0.2240\n",
            "Epoch 293/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1430 - accuracy: 0.4771 - val_loss: 16.2956 - val_accuracy: 0.2240\n",
            "Epoch 294/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1235 - accuracy: 0.4815 - val_loss: 16.2916 - val_accuracy: 0.2240\n",
            "Epoch 295/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1282 - accuracy: 0.4902 - val_loss: 16.3012 - val_accuracy: 0.2295\n",
            "Epoch 296/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1244 - accuracy: 0.4858 - val_loss: 16.2863 - val_accuracy: 0.2240\n",
            "Epoch 297/500\n",
            "459/459 [==============================] - 0s 84us/sample - loss: 1.1864 - accuracy: 0.4619 - val_loss: 16.1754 - val_accuracy: 0.2186\n",
            "Epoch 298/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1397 - accuracy: 0.4662 - val_loss: 16.1734 - val_accuracy: 0.2186\n",
            "Epoch 299/500\n",
            "459/459 [==============================] - 0s 79us/sample - loss: 1.1472 - accuracy: 0.4597 - val_loss: 16.2101 - val_accuracy: 0.2295\n",
            "Epoch 300/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1610 - accuracy: 0.4444 - val_loss: 16.2610 - val_accuracy: 0.2186\n",
            "Epoch 301/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1510 - accuracy: 0.4553 - val_loss: 16.0322 - val_accuracy: 0.2240\n",
            "Epoch 302/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1584 - accuracy: 0.4662 - val_loss: 15.9476 - val_accuracy: 0.2131\n",
            "Epoch 303/500\n",
            "459/459 [==============================] - 0s 80us/sample - loss: 1.1891 - accuracy: 0.4532 - val_loss: 15.8781 - val_accuracy: 0.2240\n",
            "Epoch 304/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1524 - accuracy: 0.4641 - val_loss: 15.8313 - val_accuracy: 0.2240\n",
            "Epoch 305/500\n",
            "459/459 [==============================] - 0s 84us/sample - loss: 1.1289 - accuracy: 0.4749 - val_loss: 15.8225 - val_accuracy: 0.2350\n",
            "Epoch 306/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1571 - accuracy: 0.4597 - val_loss: 15.9209 - val_accuracy: 0.2240\n",
            "Epoch 307/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.2006 - accuracy: 0.4662 - val_loss: 15.9178 - val_accuracy: 0.2240\n",
            "Epoch 308/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1413 - accuracy: 0.4641 - val_loss: 15.7967 - val_accuracy: 0.2295\n",
            "Epoch 309/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1558 - accuracy: 0.4553 - val_loss: 15.7802 - val_accuracy: 0.2240\n",
            "Epoch 310/500\n",
            "459/459 [==============================] - 0s 65us/sample - loss: 1.1556 - accuracy: 0.4641 - val_loss: 15.7056 - val_accuracy: 0.2131\n",
            "Epoch 311/500\n",
            "459/459 [==============================] - 0s 66us/sample - loss: 1.1492 - accuracy: 0.4662 - val_loss: 15.7182 - val_accuracy: 0.2186\n",
            "Epoch 312/500\n",
            "459/459 [==============================] - 0s 65us/sample - loss: 1.1525 - accuracy: 0.4619 - val_loss: 15.7377 - val_accuracy: 0.2295\n",
            "Epoch 313/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1143 - accuracy: 0.4728 - val_loss: 15.9199 - val_accuracy: 0.2514\n",
            "Epoch 314/500\n",
            "459/459 [==============================] - 0s 67us/sample - loss: 1.1624 - accuracy: 0.4553 - val_loss: 15.9488 - val_accuracy: 0.2514\n",
            "Epoch 315/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1355 - accuracy: 0.4662 - val_loss: 15.9046 - val_accuracy: 0.2295\n",
            "Epoch 316/500\n",
            "459/459 [==============================] - 0s 63us/sample - loss: 1.1412 - accuracy: 0.4597 - val_loss: 15.9257 - val_accuracy: 0.2350\n",
            "Epoch 317/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1712 - accuracy: 0.4510 - val_loss: 15.8885 - val_accuracy: 0.2295\n",
            "Epoch 318/500\n",
            "459/459 [==============================] - 0s 65us/sample - loss: 1.1380 - accuracy: 0.4662 - val_loss: 15.9249 - val_accuracy: 0.2295\n",
            "Epoch 319/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1358 - accuracy: 0.4619 - val_loss: 15.8789 - val_accuracy: 0.2295\n",
            "Epoch 320/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1751 - accuracy: 0.4662 - val_loss: 15.9622 - val_accuracy: 0.2240\n",
            "Epoch 321/500\n",
            "459/459 [==============================] - 0s 83us/sample - loss: 1.1719 - accuracy: 0.4837 - val_loss: 15.9721 - val_accuracy: 0.2350\n",
            "Epoch 322/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1232 - accuracy: 0.4858 - val_loss: 16.0060 - val_accuracy: 0.2459\n",
            "Epoch 323/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1375 - accuracy: 0.4749 - val_loss: 16.0127 - val_accuracy: 0.2404\n",
            "Epoch 324/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1271 - accuracy: 0.4728 - val_loss: 15.9958 - val_accuracy: 0.2295\n",
            "Epoch 325/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1251 - accuracy: 0.4793 - val_loss: 15.8344 - val_accuracy: 0.2186\n",
            "Epoch 326/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1412 - accuracy: 0.4553 - val_loss: 15.7465 - val_accuracy: 0.2240\n",
            "Epoch 327/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1667 - accuracy: 0.4815 - val_loss: 15.7506 - val_accuracy: 0.2240\n",
            "Epoch 328/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1588 - accuracy: 0.4619 - val_loss: 15.7466 - val_accuracy: 0.2295\n",
            "Epoch 329/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1840 - accuracy: 0.4379 - val_loss: 15.7702 - val_accuracy: 0.2240\n",
            "Epoch 330/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1358 - accuracy: 0.4662 - val_loss: 15.7294 - val_accuracy: 0.2350\n",
            "Epoch 331/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1180 - accuracy: 0.4837 - val_loss: 15.9842 - val_accuracy: 0.2350\n",
            "Epoch 332/500\n",
            "459/459 [==============================] - 0s 79us/sample - loss: 1.1734 - accuracy: 0.4597 - val_loss: 16.0606 - val_accuracy: 0.2350\n",
            "Epoch 333/500\n",
            "459/459 [==============================] - 0s 82us/sample - loss: 1.1278 - accuracy: 0.4815 - val_loss: 16.0177 - val_accuracy: 0.2240\n",
            "Epoch 334/500\n",
            "459/459 [==============================] - 0s 93us/sample - loss: 1.1550 - accuracy: 0.4662 - val_loss: 15.8988 - val_accuracy: 0.2295\n",
            "Epoch 335/500\n",
            "459/459 [==============================] - 0s 93us/sample - loss: 1.1263 - accuracy: 0.4793 - val_loss: 15.8609 - val_accuracy: 0.2350\n",
            "Epoch 336/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1116 - accuracy: 0.4728 - val_loss: 15.8357 - val_accuracy: 0.2240\n",
            "Epoch 337/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1564 - accuracy: 0.4619 - val_loss: 16.0961 - val_accuracy: 0.2240\n",
            "Epoch 338/500\n",
            "459/459 [==============================] - 0s 81us/sample - loss: 1.1781 - accuracy: 0.4575 - val_loss: 16.8670 - val_accuracy: 0.2295\n",
            "Epoch 339/500\n",
            "459/459 [==============================] - 0s 79us/sample - loss: 1.1818 - accuracy: 0.4401 - val_loss: 16.2579 - val_accuracy: 0.2295\n",
            "Epoch 340/500\n",
            "459/459 [==============================] - 0s 106us/sample - loss: 1.1427 - accuracy: 0.4706 - val_loss: 15.9450 - val_accuracy: 0.2350\n",
            "Epoch 341/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1500 - accuracy: 0.4815 - val_loss: 15.8675 - val_accuracy: 0.2459\n",
            "Epoch 342/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1420 - accuracy: 0.4837 - val_loss: 15.9276 - val_accuracy: 0.2404\n",
            "Epoch 343/500\n",
            "459/459 [==============================] - 0s 85us/sample - loss: 1.1430 - accuracy: 0.4662 - val_loss: 15.9891 - val_accuracy: 0.2350\n",
            "Epoch 344/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.2018 - accuracy: 0.4619 - val_loss: 16.0697 - val_accuracy: 0.2295\n",
            "Epoch 345/500\n",
            "459/459 [==============================] - 0s 98us/sample - loss: 1.1464 - accuracy: 0.4728 - val_loss: 15.9716 - val_accuracy: 0.2350\n",
            "Epoch 346/500\n",
            "459/459 [==============================] - 0s 93us/sample - loss: 1.1423 - accuracy: 0.4706 - val_loss: 16.4143 - val_accuracy: 0.2350\n",
            "Epoch 347/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1505 - accuracy: 0.4706 - val_loss: 16.6453 - val_accuracy: 0.2404\n",
            "Epoch 348/500\n",
            "459/459 [==============================] - 0s 89us/sample - loss: 1.1299 - accuracy: 0.4749 - val_loss: 16.4912 - val_accuracy: 0.2404\n",
            "Epoch 349/500\n",
            "459/459 [==============================] - 0s 83us/sample - loss: 1.1913 - accuracy: 0.4553 - val_loss: 16.4382 - val_accuracy: 0.2404\n",
            "Epoch 350/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1393 - accuracy: 0.4553 - val_loss: 16.4098 - val_accuracy: 0.2404\n",
            "Epoch 351/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1355 - accuracy: 0.4619 - val_loss: 16.4311 - val_accuracy: 0.2350\n",
            "Epoch 352/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1394 - accuracy: 0.4771 - val_loss: 16.4382 - val_accuracy: 0.2186\n",
            "Epoch 353/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1485 - accuracy: 0.4575 - val_loss: 16.3888 - val_accuracy: 0.2186\n",
            "Epoch 354/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1509 - accuracy: 0.4749 - val_loss: 16.3508 - val_accuracy: 0.2131\n",
            "Epoch 355/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1392 - accuracy: 0.4619 - val_loss: 16.2936 - val_accuracy: 0.2295\n",
            "Epoch 356/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1349 - accuracy: 0.4749 - val_loss: 16.1513 - val_accuracy: 0.2295\n",
            "Epoch 357/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1370 - accuracy: 0.4488 - val_loss: 15.9327 - val_accuracy: 0.2404\n",
            "Epoch 358/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1241 - accuracy: 0.4684 - val_loss: 15.8164 - val_accuracy: 0.2240\n",
            "Epoch 359/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1658 - accuracy: 0.4488 - val_loss: 16.0295 - val_accuracy: 0.2295\n",
            "Epoch 360/500\n",
            "459/459 [==============================] - 0s 84us/sample - loss: 1.1407 - accuracy: 0.4532 - val_loss: 16.0982 - val_accuracy: 0.2350\n",
            "Epoch 361/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1635 - accuracy: 0.4270 - val_loss: 16.0554 - val_accuracy: 0.2404\n",
            "Epoch 362/500\n",
            "459/459 [==============================] - 0s 91us/sample - loss: 1.1701 - accuracy: 0.4510 - val_loss: 16.1900 - val_accuracy: 0.2404\n",
            "Epoch 363/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1708 - accuracy: 0.4466 - val_loss: 16.1753 - val_accuracy: 0.2240\n",
            "Epoch 364/500\n",
            "459/459 [==============================] - 0s 67us/sample - loss: 1.1870 - accuracy: 0.4270 - val_loss: 16.0634 - val_accuracy: 0.2295\n",
            "Epoch 365/500\n",
            "459/459 [==============================] - 0s 65us/sample - loss: 1.1515 - accuracy: 0.4423 - val_loss: 16.0607 - val_accuracy: 0.2240\n",
            "Epoch 366/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1370 - accuracy: 0.4532 - val_loss: 16.1045 - val_accuracy: 0.2404\n",
            "Epoch 367/500\n",
            "459/459 [==============================] - 0s 67us/sample - loss: 1.1580 - accuracy: 0.4619 - val_loss: 16.2849 - val_accuracy: 0.2568\n",
            "Epoch 368/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1362 - accuracy: 0.4706 - val_loss: 16.0963 - val_accuracy: 0.2459\n",
            "Epoch 369/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1522 - accuracy: 0.4553 - val_loss: 15.7456 - val_accuracy: 0.2240\n",
            "Epoch 370/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1304 - accuracy: 0.4619 - val_loss: 15.7215 - val_accuracy: 0.2295\n",
            "Epoch 371/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1802 - accuracy: 0.4466 - val_loss: 15.6922 - val_accuracy: 0.2350\n",
            "Epoch 372/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1785 - accuracy: 0.4466 - val_loss: 15.7899 - val_accuracy: 0.2514\n",
            "Epoch 373/500\n",
            "459/459 [==============================] - 0s 67us/sample - loss: 1.1575 - accuracy: 0.4510 - val_loss: 15.7786 - val_accuracy: 0.2459\n",
            "Epoch 374/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1566 - accuracy: 0.4510 - val_loss: 15.5176 - val_accuracy: 0.2404\n",
            "Epoch 375/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1396 - accuracy: 0.4619 - val_loss: 15.4460 - val_accuracy: 0.2404\n",
            "Epoch 376/500\n",
            "459/459 [==============================] - 0s 65us/sample - loss: 1.1524 - accuracy: 0.4553 - val_loss: 15.6539 - val_accuracy: 0.2404\n",
            "Epoch 377/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1669 - accuracy: 0.4510 - val_loss: 15.6059 - val_accuracy: 0.2404\n",
            "Epoch 378/500\n",
            "459/459 [==============================] - 0s 83us/sample - loss: 1.1844 - accuracy: 0.4597 - val_loss: 15.5735 - val_accuracy: 0.2404\n",
            "Epoch 379/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1414 - accuracy: 0.4532 - val_loss: 15.6089 - val_accuracy: 0.2350\n",
            "Epoch 380/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1283 - accuracy: 0.4793 - val_loss: 15.5737 - val_accuracy: 0.2404\n",
            "Epoch 381/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1220 - accuracy: 0.4902 - val_loss: 15.6504 - val_accuracy: 0.2350\n",
            "Epoch 382/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1458 - accuracy: 0.4641 - val_loss: 15.7302 - val_accuracy: 0.2295\n",
            "Epoch 383/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1568 - accuracy: 0.4619 - val_loss: 15.7964 - val_accuracy: 0.2295\n",
            "Epoch 384/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1598 - accuracy: 0.4488 - val_loss: 15.6229 - val_accuracy: 0.2295\n",
            "Epoch 385/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1619 - accuracy: 0.4553 - val_loss: 15.5072 - val_accuracy: 0.2350\n",
            "Epoch 386/500\n",
            "459/459 [==============================] - 0s 81us/sample - loss: 1.1251 - accuracy: 0.4728 - val_loss: 15.5186 - val_accuracy: 0.2350\n",
            "Epoch 387/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1561 - accuracy: 0.4597 - val_loss: 15.5831 - val_accuracy: 0.2295\n",
            "Epoch 388/500\n",
            "459/459 [==============================] - 0s 65us/sample - loss: 1.1672 - accuracy: 0.4401 - val_loss: 15.6349 - val_accuracy: 0.2404\n",
            "Epoch 389/500\n",
            "459/459 [==============================] - 0s 66us/sample - loss: 1.1654 - accuracy: 0.4575 - val_loss: 15.7150 - val_accuracy: 0.2459\n",
            "Epoch 390/500\n",
            "459/459 [==============================] - 0s 87us/sample - loss: 1.1249 - accuracy: 0.4641 - val_loss: 15.8263 - val_accuracy: 0.2350\n",
            "Epoch 391/500\n",
            "459/459 [==============================] - 0s 94us/sample - loss: 1.1483 - accuracy: 0.4532 - val_loss: 15.7581 - val_accuracy: 0.2404\n",
            "Epoch 392/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1273 - accuracy: 0.4684 - val_loss: 15.6720 - val_accuracy: 0.2350\n",
            "Epoch 393/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1678 - accuracy: 0.4357 - val_loss: 15.7372 - val_accuracy: 0.2404\n",
            "Epoch 394/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1622 - accuracy: 0.4444 - val_loss: 15.8760 - val_accuracy: 0.2295\n",
            "Epoch 395/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1596 - accuracy: 0.4510 - val_loss: 15.9312 - val_accuracy: 0.2295\n",
            "Epoch 396/500\n",
            "459/459 [==============================] - 0s 66us/sample - loss: 1.1304 - accuracy: 0.4706 - val_loss: 16.0444 - val_accuracy: 0.2459\n",
            "Epoch 397/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1536 - accuracy: 0.4684 - val_loss: 15.9489 - val_accuracy: 0.2404\n",
            "Epoch 398/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1387 - accuracy: 0.4815 - val_loss: 16.2525 - val_accuracy: 0.2404\n",
            "Epoch 399/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1286 - accuracy: 0.4880 - val_loss: 16.2320 - val_accuracy: 0.2350\n",
            "Epoch 400/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1865 - accuracy: 0.4466 - val_loss: 16.4600 - val_accuracy: 0.2404\n",
            "Epoch 401/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1450 - accuracy: 0.4619 - val_loss: 16.7888 - val_accuracy: 0.2459\n",
            "Epoch 402/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1586 - accuracy: 0.4619 - val_loss: 16.7475 - val_accuracy: 0.2459\n",
            "Epoch 403/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1748 - accuracy: 0.4532 - val_loss: 16.8072 - val_accuracy: 0.2514\n",
            "Epoch 404/500\n",
            "459/459 [==============================] - 0s 67us/sample - loss: 1.1685 - accuracy: 0.4423 - val_loss: 16.7231 - val_accuracy: 0.2350\n",
            "Epoch 405/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1782 - accuracy: 0.4553 - val_loss: 16.4660 - val_accuracy: 0.2350\n",
            "Epoch 406/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1533 - accuracy: 0.4684 - val_loss: 15.8173 - val_accuracy: 0.2459\n",
            "Epoch 407/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1616 - accuracy: 0.4553 - val_loss: 15.7377 - val_accuracy: 0.2404\n",
            "Epoch 408/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1715 - accuracy: 0.4706 - val_loss: 15.8193 - val_accuracy: 0.2404\n",
            "Epoch 409/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1343 - accuracy: 0.4597 - val_loss: 16.4533 - val_accuracy: 0.2568\n",
            "Epoch 410/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1375 - accuracy: 0.4662 - val_loss: 16.7070 - val_accuracy: 0.2514\n",
            "Epoch 411/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1820 - accuracy: 0.4597 - val_loss: 17.1223 - val_accuracy: 0.2568\n",
            "Epoch 412/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1494 - accuracy: 0.4749 - val_loss: 17.2524 - val_accuracy: 0.2404\n",
            "Epoch 413/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1300 - accuracy: 0.4771 - val_loss: 16.8524 - val_accuracy: 0.2459\n",
            "Epoch 414/500\n",
            "459/459 [==============================] - 0s 81us/sample - loss: 1.1161 - accuracy: 0.4815 - val_loss: 16.7781 - val_accuracy: 0.2404\n",
            "Epoch 415/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1838 - accuracy: 0.4553 - val_loss: 16.7031 - val_accuracy: 0.2459\n",
            "Epoch 416/500\n",
            "459/459 [==============================] - 0s 84us/sample - loss: 1.1302 - accuracy: 0.4771 - val_loss: 16.6897 - val_accuracy: 0.2459\n",
            "Epoch 417/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1578 - accuracy: 0.4662 - val_loss: 16.5975 - val_accuracy: 0.2404\n",
            "Epoch 418/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1646 - accuracy: 0.4619 - val_loss: 16.7312 - val_accuracy: 0.2404\n",
            "Epoch 419/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1609 - accuracy: 0.4597 - val_loss: 16.7618 - val_accuracy: 0.2240\n",
            "Epoch 420/500\n",
            "459/459 [==============================] - 0s 99us/sample - loss: 1.1308 - accuracy: 0.4553 - val_loss: 17.4074 - val_accuracy: 0.2459\n",
            "Epoch 421/500\n",
            "459/459 [==============================] - 0s 92us/sample - loss: 1.1413 - accuracy: 0.4619 - val_loss: 18.0204 - val_accuracy: 0.2404\n",
            "Epoch 422/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1595 - accuracy: 0.4684 - val_loss: 17.5812 - val_accuracy: 0.2404\n",
            "Epoch 423/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1629 - accuracy: 0.4597 - val_loss: 17.1522 - val_accuracy: 0.2295\n",
            "Epoch 424/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1147 - accuracy: 0.4771 - val_loss: 17.1313 - val_accuracy: 0.2404\n",
            "Epoch 425/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1627 - accuracy: 0.4597 - val_loss: 17.2041 - val_accuracy: 0.2459\n",
            "Epoch 426/500\n",
            "459/459 [==============================] - 0s 66us/sample - loss: 1.1252 - accuracy: 0.4858 - val_loss: 16.9992 - val_accuracy: 0.2350\n",
            "Epoch 427/500\n",
            "459/459 [==============================] - 0s 64us/sample - loss: 1.1544 - accuracy: 0.4662 - val_loss: 16.8648 - val_accuracy: 0.2459\n",
            "Epoch 428/500\n",
            "459/459 [==============================] - 0s 66us/sample - loss: 1.1393 - accuracy: 0.4706 - val_loss: 16.8565 - val_accuracy: 0.2295\n",
            "Epoch 429/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1266 - accuracy: 0.4815 - val_loss: 16.8615 - val_accuracy: 0.2295\n",
            "Epoch 430/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1424 - accuracy: 0.4684 - val_loss: 16.6056 - val_accuracy: 0.2295\n",
            "Epoch 431/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1531 - accuracy: 0.4619 - val_loss: 16.4828 - val_accuracy: 0.2459\n",
            "Epoch 432/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1607 - accuracy: 0.4532 - val_loss: 16.5735 - val_accuracy: 0.2350\n",
            "Epoch 433/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1466 - accuracy: 0.4553 - val_loss: 16.6319 - val_accuracy: 0.2404\n",
            "Epoch 434/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1168 - accuracy: 0.4858 - val_loss: 16.8977 - val_accuracy: 0.2295\n",
            "Epoch 435/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1354 - accuracy: 0.4575 - val_loss: 17.0564 - val_accuracy: 0.2295\n",
            "Epoch 436/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1485 - accuracy: 0.4575 - val_loss: 16.9501 - val_accuracy: 0.2350\n",
            "Epoch 437/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1825 - accuracy: 0.4684 - val_loss: 16.8317 - val_accuracy: 0.2186\n",
            "Epoch 438/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.2083 - accuracy: 0.4466 - val_loss: 17.1825 - val_accuracy: 0.2350\n",
            "Epoch 439/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1793 - accuracy: 0.4357 - val_loss: 18.4880 - val_accuracy: 0.2404\n",
            "Epoch 440/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1714 - accuracy: 0.4488 - val_loss: 17.8875 - val_accuracy: 0.2404\n",
            "Epoch 441/500\n",
            "459/459 [==============================] - 0s 79us/sample - loss: 1.1216 - accuracy: 0.4662 - val_loss: 17.2613 - val_accuracy: 0.2459\n",
            "Epoch 442/500\n",
            "459/459 [==============================] - 0s 79us/sample - loss: 1.1327 - accuracy: 0.4728 - val_loss: 16.8816 - val_accuracy: 0.2404\n",
            "Epoch 443/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1401 - accuracy: 0.4401 - val_loss: 16.8415 - val_accuracy: 0.2350\n",
            "Epoch 444/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1723 - accuracy: 0.4684 - val_loss: 16.8131 - val_accuracy: 0.2295\n",
            "Epoch 445/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1878 - accuracy: 0.4488 - val_loss: 16.9124 - val_accuracy: 0.2568\n",
            "Epoch 446/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1176 - accuracy: 0.4837 - val_loss: 16.9656 - val_accuracy: 0.2568\n",
            "Epoch 447/500\n",
            "459/459 [==============================] - 0s 65us/sample - loss: 1.1659 - accuracy: 0.4684 - val_loss: 16.8772 - val_accuracy: 0.2623\n",
            "Epoch 448/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1316 - accuracy: 0.4880 - val_loss: 16.8327 - val_accuracy: 0.2568\n",
            "Epoch 449/500\n",
            "459/459 [==============================] - 0s 95us/sample - loss: 1.1764 - accuracy: 0.4641 - val_loss: 16.7785 - val_accuracy: 0.2404\n",
            "Epoch 450/500\n",
            "459/459 [==============================] - 0s 87us/sample - loss: 1.1531 - accuracy: 0.4532 - val_loss: 16.7072 - val_accuracy: 0.2350\n",
            "Epoch 451/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1283 - accuracy: 0.4641 - val_loss: 16.6371 - val_accuracy: 0.2568\n",
            "Epoch 452/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1447 - accuracy: 0.4641 - val_loss: 16.5446 - val_accuracy: 0.2459\n",
            "Epoch 453/500\n",
            "459/459 [==============================] - 0s 86us/sample - loss: 1.1679 - accuracy: 0.4466 - val_loss: 16.7782 - val_accuracy: 0.2514\n",
            "Epoch 454/500\n",
            "459/459 [==============================] - 0s 80us/sample - loss: 1.1548 - accuracy: 0.4684 - val_loss: 17.0425 - val_accuracy: 0.2295\n",
            "Epoch 455/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1973 - accuracy: 0.4619 - val_loss: 17.5028 - val_accuracy: 0.2295\n",
            "Epoch 456/500\n",
            "459/459 [==============================] - 0s 80us/sample - loss: 1.1704 - accuracy: 0.4510 - val_loss: 17.3922 - val_accuracy: 0.2350\n",
            "Epoch 457/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1606 - accuracy: 0.4488 - val_loss: 17.4023 - val_accuracy: 0.2350\n",
            "Epoch 458/500\n",
            "459/459 [==============================] - 0s 85us/sample - loss: 1.1463 - accuracy: 0.4510 - val_loss: 17.2226 - val_accuracy: 0.2350\n",
            "Epoch 459/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1794 - accuracy: 0.4423 - val_loss: 17.2913 - val_accuracy: 0.2295\n",
            "Epoch 460/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1563 - accuracy: 0.4619 - val_loss: 17.2124 - val_accuracy: 0.2131\n",
            "Epoch 461/500\n",
            "459/459 [==============================] - 0s 75us/sample - loss: 1.1473 - accuracy: 0.4662 - val_loss: 17.1029 - val_accuracy: 0.2240\n",
            "Epoch 462/500\n",
            "459/459 [==============================] - 0s 80us/sample - loss: 1.1565 - accuracy: 0.4706 - val_loss: 17.0353 - val_accuracy: 0.2186\n",
            "Epoch 463/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1220 - accuracy: 0.4662 - val_loss: 17.0487 - val_accuracy: 0.2131\n",
            "Epoch 464/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1246 - accuracy: 0.4728 - val_loss: 16.9903 - val_accuracy: 0.2240\n",
            "Epoch 465/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1438 - accuracy: 0.4510 - val_loss: 16.9257 - val_accuracy: 0.2295\n",
            "Epoch 466/500\n",
            "459/459 [==============================] - 0s 83us/sample - loss: 1.1648 - accuracy: 0.4553 - val_loss: 16.7453 - val_accuracy: 0.2295\n",
            "Epoch 467/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1557 - accuracy: 0.4684 - val_loss: 16.4883 - val_accuracy: 0.2295\n",
            "Epoch 468/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1448 - accuracy: 0.4575 - val_loss: 16.5473 - val_accuracy: 0.2404\n",
            "Epoch 469/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1181 - accuracy: 0.4749 - val_loss: 16.9088 - val_accuracy: 0.2404\n",
            "Epoch 470/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1424 - accuracy: 0.4662 - val_loss: 16.9218 - val_accuracy: 0.2240\n",
            "Epoch 471/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1293 - accuracy: 0.4662 - val_loss: 17.6537 - val_accuracy: 0.2295\n",
            "Epoch 472/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1245 - accuracy: 0.4706 - val_loss: 18.1252 - val_accuracy: 0.2295\n",
            "Epoch 473/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1864 - accuracy: 0.4597 - val_loss: 18.5376 - val_accuracy: 0.2404\n",
            "Epoch 474/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1524 - accuracy: 0.4641 - val_loss: 17.5809 - val_accuracy: 0.2240\n",
            "Epoch 475/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1097 - accuracy: 0.4946 - val_loss: 17.4066 - val_accuracy: 0.2404\n",
            "Epoch 476/500\n",
            "459/459 [==============================] - 0s 96us/sample - loss: 1.1394 - accuracy: 0.4597 - val_loss: 17.2540 - val_accuracy: 0.2350\n",
            "Epoch 477/500\n",
            "459/459 [==============================] - 0s 90us/sample - loss: 1.1492 - accuracy: 0.4444 - val_loss: 17.1041 - val_accuracy: 0.2295\n",
            "Epoch 478/500\n",
            "459/459 [==============================] - 0s 82us/sample - loss: 1.1659 - accuracy: 0.4510 - val_loss: 16.9903 - val_accuracy: 0.2350\n",
            "Epoch 479/500\n",
            "459/459 [==============================] - 0s 84us/sample - loss: 1.1188 - accuracy: 0.4662 - val_loss: 16.9708 - val_accuracy: 0.2295\n",
            "Epoch 480/500\n",
            "459/459 [==============================] - 0s 81us/sample - loss: 1.1181 - accuracy: 0.4706 - val_loss: 16.9749 - val_accuracy: 0.2295\n",
            "Epoch 481/500\n",
            "459/459 [==============================] - 0s 68us/sample - loss: 1.1456 - accuracy: 0.4684 - val_loss: 16.9778 - val_accuracy: 0.2295\n",
            "Epoch 482/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1499 - accuracy: 0.4466 - val_loss: 16.9702 - val_accuracy: 0.2404\n",
            "Epoch 483/500\n",
            "459/459 [==============================] - 0s 84us/sample - loss: 1.1780 - accuracy: 0.4749 - val_loss: 16.9415 - val_accuracy: 0.2623\n",
            "Epoch 484/500\n",
            "459/459 [==============================] - 0s 70us/sample - loss: 1.1511 - accuracy: 0.4597 - val_loss: 16.9163 - val_accuracy: 0.2514\n",
            "Epoch 485/500\n",
            "459/459 [==============================] - 0s 77us/sample - loss: 1.1495 - accuracy: 0.4749 - val_loss: 16.8832 - val_accuracy: 0.2514\n",
            "Epoch 486/500\n",
            "459/459 [==============================] - 0s 66us/sample - loss: 1.1507 - accuracy: 0.4749 - val_loss: 16.7986 - val_accuracy: 0.2404\n",
            "Epoch 487/500\n",
            "459/459 [==============================] - 0s 87us/sample - loss: 1.1339 - accuracy: 0.4728 - val_loss: 16.9402 - val_accuracy: 0.2623\n",
            "Epoch 488/500\n",
            "459/459 [==============================] - 0s 81us/sample - loss: 1.1546 - accuracy: 0.4575 - val_loss: 16.9481 - val_accuracy: 0.2459\n",
            "Epoch 489/500\n",
            "459/459 [==============================] - 0s 71us/sample - loss: 1.1395 - accuracy: 0.4641 - val_loss: 16.9059 - val_accuracy: 0.2404\n",
            "Epoch 490/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1195 - accuracy: 0.4815 - val_loss: 16.8230 - val_accuracy: 0.2350\n",
            "Epoch 491/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1449 - accuracy: 0.4510 - val_loss: 17.0796 - val_accuracy: 0.2514\n",
            "Epoch 492/500\n",
            "459/459 [==============================] - 0s 72us/sample - loss: 1.1530 - accuracy: 0.4532 - val_loss: 17.3561 - val_accuracy: 0.2459\n",
            "Epoch 493/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1410 - accuracy: 0.4619 - val_loss: 17.1322 - val_accuracy: 0.2404\n",
            "Epoch 494/500\n",
            "459/459 [==============================] - 0s 76us/sample - loss: 1.1845 - accuracy: 0.4641 - val_loss: 17.3840 - val_accuracy: 0.2459\n",
            "Epoch 495/500\n",
            "459/459 [==============================] - 0s 67us/sample - loss: 1.1661 - accuracy: 0.4553 - val_loss: 17.7294 - val_accuracy: 0.2459\n",
            "Epoch 496/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1516 - accuracy: 0.4532 - val_loss: 17.5332 - val_accuracy: 0.2459\n",
            "Epoch 497/500\n",
            "459/459 [==============================] - 0s 69us/sample - loss: 1.1536 - accuracy: 0.4575 - val_loss: 17.4028 - val_accuracy: 0.2404\n",
            "Epoch 498/500\n",
            "459/459 [==============================] - 0s 78us/sample - loss: 1.1258 - accuracy: 0.4684 - val_loss: 17.2455 - val_accuracy: 0.2350\n",
            "Epoch 499/500\n",
            "459/459 [==============================] - 0s 73us/sample - loss: 1.1857 - accuracy: 0.4488 - val_loss: 17.0565 - val_accuracy: 0.2350\n",
            "Epoch 500/500\n",
            "459/459 [==============================] - 0s 74us/sample - loss: 1.1615 - accuracy: 0.4662 - val_loss: 17.1388 - val_accuracy: 0.2295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mlo9hZQUAV8Z",
        "colab_type": "text"
      },
      "source": [
        "MODEL BUILDING CNN, RNN, AND LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1CaznCWqIam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(1000, 16),\n",
        "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC_Y1_E1O036",
        "colab_type": "code",
        "outputId": "6abe3c34-4c63-40c8-b4dd-8514ad149b74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 16)          16000     \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, None, 128)         10368     \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                2064      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 85        \n",
            "=================================================================\n",
            "Total params: 28,517\n",
            "Trainable params: 28,517\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfJHoK5GPDfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss = 'sparse_categorical_crossentropy', \n",
        "              optimizer = 'adam', \n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kNZ4jo4O5C-",
        "colab_type": "code",
        "outputId": "5658af0a-5a9d-4dc6-9d4d-1ed5080a6f07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "NUM_EPOCHS = 300\n",
        "history = model.fit(train_vec, training_labels,\n",
        "                    validation_data = (val_vec, validation_labels),\n",
        "                    epochs = NUM_EPOCHS, \n",
        "                    callbacks = [checkpoint, callbacks])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 459 samples, validate on 183 samples\n",
            "Epoch 1/300\n",
            "459/459 [==============================] - 1s 2ms/sample - loss: 1.6113 - accuracy: 0.2026 - val_loss: 1.6116 - val_accuracy: 0.2022\n",
            "Epoch 2/300\n",
            "459/459 [==============================] - 0s 263us/sample - loss: 1.6107 - accuracy: 0.2026 - val_loss: 1.6113 - val_accuracy: 0.2022\n",
            "Epoch 3/300\n",
            "459/459 [==============================] - 0s 269us/sample - loss: 1.6104 - accuracy: 0.2026 - val_loss: 1.6110 - val_accuracy: 0.2022\n",
            "Epoch 4/300\n",
            "459/459 [==============================] - 0s 290us/sample - loss: 1.6102 - accuracy: 0.2026 - val_loss: 1.6109 - val_accuracy: 0.2022\n",
            "Epoch 5/300\n",
            "459/459 [==============================] - 0s 295us/sample - loss: 1.6102 - accuracy: 0.2026 - val_loss: 1.6106 - val_accuracy: 0.2022\n",
            "Epoch 6/300\n",
            "459/459 [==============================] - 0s 298us/sample - loss: 1.6101 - accuracy: 0.2026 - val_loss: 1.6105 - val_accuracy: 0.2022\n",
            "Epoch 7/300\n",
            "459/459 [==============================] - 0s 305us/sample - loss: 1.6100 - accuracy: 0.2026 - val_loss: 1.6104 - val_accuracy: 0.2022\n",
            "Epoch 8/300\n",
            "459/459 [==============================] - 0s 281us/sample - loss: 1.6099 - accuracy: 0.2026 - val_loss: 1.6102 - val_accuracy: 0.2022\n",
            "Epoch 9/300\n",
            "459/459 [==============================] - 0s 297us/sample - loss: 1.6099 - accuracy: 0.2026 - val_loss: 1.6101 - val_accuracy: 0.2022\n",
            "Epoch 10/300\n",
            "459/459 [==============================] - 0s 272us/sample - loss: 1.6098 - accuracy: 0.2026 - val_loss: 1.6100 - val_accuracy: 0.2022\n",
            "Epoch 11/300\n",
            "459/459 [==============================] - 0s 267us/sample - loss: 1.6098 - accuracy: 0.2026 - val_loss: 1.6100 - val_accuracy: 0.2022\n",
            "Epoch 12/300\n",
            "459/459 [==============================] - 0s 267us/sample - loss: 1.6098 - accuracy: 0.2026 - val_loss: 1.6100 - val_accuracy: 0.2022\n",
            "Epoch 13/300\n",
            "459/459 [==============================] - 0s 260us/sample - loss: 1.6097 - accuracy: 0.2026 - val_loss: 1.6099 - val_accuracy: 0.2022\n",
            "Epoch 14/300\n",
            "459/459 [==============================] - 0s 294us/sample - loss: 1.6096 - accuracy: 0.2026 - val_loss: 1.6098 - val_accuracy: 0.2022\n",
            "Epoch 15/300\n",
            "459/459 [==============================] - 0s 280us/sample - loss: 1.6096 - accuracy: 0.2026 - val_loss: 1.6098 - val_accuracy: 0.2022\n",
            "Epoch 16/300\n",
            "459/459 [==============================] - 0s 280us/sample - loss: 1.6097 - accuracy: 0.2026 - val_loss: 1.6099 - val_accuracy: 0.2022\n",
            "Epoch 17/300\n",
            "459/459 [==============================] - 0s 278us/sample - loss: 1.6096 - accuracy: 0.2026 - val_loss: 1.6098 - val_accuracy: 0.2022\n",
            "Epoch 18/300\n",
            "459/459 [==============================] - 0s 295us/sample - loss: 1.6097 - accuracy: 0.2026 - val_loss: 1.6096 - val_accuracy: 0.2022\n",
            "Epoch 19/300\n",
            "459/459 [==============================] - 0s 292us/sample - loss: 1.6096 - accuracy: 0.2026 - val_loss: 1.6097 - val_accuracy: 0.2022\n",
            "Epoch 20/300\n",
            "459/459 [==============================] - 0s 259us/sample - loss: 1.6097 - accuracy: 0.2026 - val_loss: 1.6095 - val_accuracy: 0.2022\n",
            "Epoch 21/300\n",
            "459/459 [==============================] - 0s 259us/sample - loss: 1.6096 - accuracy: 0.2026 - val_loss: 1.6095 - val_accuracy: 0.2022\n",
            "Epoch 22/300\n",
            "459/459 [==============================] - 0s 293us/sample - loss: 1.6097 - accuracy: 0.2026 - val_loss: 1.6096 - val_accuracy: 0.2022\n",
            "Epoch 23/300\n",
            "459/459 [==============================] - 0s 258us/sample - loss: 1.6095 - accuracy: 0.2026 - val_loss: 1.6096 - val_accuracy: 0.2022\n",
            "Epoch 24/300\n",
            "459/459 [==============================] - 0s 279us/sample - loss: 1.6094 - accuracy: 0.2026 - val_loss: 1.6096 - val_accuracy: 0.2022\n",
            "Epoch 25/300\n",
            "459/459 [==============================] - 0s 270us/sample - loss: 1.6095 - accuracy: 0.2026 - val_loss: 1.6097 - val_accuracy: 0.2022\n",
            "Epoch 26/300\n",
            "459/459 [==============================] - 0s 296us/sample - loss: 1.6095 - accuracy: 0.2026 - val_loss: 1.6096 - val_accuracy: 0.2022\n",
            "Epoch 27/300\n",
            "459/459 [==============================] - 0s 270us/sample - loss: 1.6094 - accuracy: 0.2026 - val_loss: 1.6097 - val_accuracy: 0.2022\n",
            "Epoch 28/300\n",
            "459/459 [==============================] - 0s 276us/sample - loss: 1.6094 - accuracy: 0.2026 - val_loss: 1.6096 - val_accuracy: 0.2022\n",
            "Epoch 29/300\n",
            "459/459 [==============================] - 0s 282us/sample - loss: 1.6094 - accuracy: 0.2026 - val_loss: 1.6096 - val_accuracy: 0.2022\n",
            "Epoch 30/300\n",
            "459/459 [==============================] - 0s 290us/sample - loss: 1.6093 - accuracy: 0.2026 - val_loss: 1.6095 - val_accuracy: 0.2022\n",
            "Epoch 31/300\n",
            "459/459 [==============================] - 0s 266us/sample - loss: 1.6092 - accuracy: 0.2026 - val_loss: 1.6096 - val_accuracy: 0.2022\n",
            "Epoch 32/300\n",
            "459/459 [==============================] - 0s 298us/sample - loss: 1.6091 - accuracy: 0.2026 - val_loss: 1.6094 - val_accuracy: 0.2022\n",
            "Epoch 33/300\n",
            "459/459 [==============================] - 0s 343us/sample - loss: 1.6090 - accuracy: 0.2026 - val_loss: 1.6093 - val_accuracy: 0.2022\n",
            "Epoch 34/300\n",
            "459/459 [==============================] - 0s 268us/sample - loss: 1.6086 - accuracy: 0.2397 - val_loss: 1.6093 - val_accuracy: 0.2240\n",
            "Epoch 35/300\n",
            "459/459 [==============================] - 0s 255us/sample - loss: 1.6083 - accuracy: 0.2549 - val_loss: 1.6091 - val_accuracy: 0.2350\n",
            "Epoch 36/300\n",
            "459/459 [==============================] - 0s 275us/sample - loss: 1.6079 - accuracy: 0.2309 - val_loss: 1.6088 - val_accuracy: 0.2022\n",
            "Epoch 37/300\n",
            "459/459 [==============================] - 0s 251us/sample - loss: 1.6073 - accuracy: 0.2026 - val_loss: 1.6086 - val_accuracy: 0.2022\n",
            "Epoch 38/300\n",
            "459/459 [==============================] - 0s 293us/sample - loss: 1.6065 - accuracy: 0.2026 - val_loss: 1.6082 - val_accuracy: 0.2022\n",
            "Epoch 39/300\n",
            "459/459 [==============================] - 0s 304us/sample - loss: 1.6055 - accuracy: 0.2135 - val_loss: 1.6077 - val_accuracy: 0.2131\n",
            "Epoch 40/300\n",
            "459/459 [==============================] - 0s 298us/sample - loss: 1.6050 - accuracy: 0.2026 - val_loss: 1.6073 - val_accuracy: 0.2022\n",
            "Epoch 41/300\n",
            "459/459 [==============================] - 0s 274us/sample - loss: 1.6041 - accuracy: 0.2527 - val_loss: 1.6071 - val_accuracy: 0.3005\n",
            "Epoch 42/300\n",
            "459/459 [==============================] - 0s 284us/sample - loss: 1.6033 - accuracy: 0.3028 - val_loss: 1.6069 - val_accuracy: 0.2787\n",
            "Epoch 43/300\n",
            "459/459 [==============================] - 0s 287us/sample - loss: 1.6027 - accuracy: 0.2832 - val_loss: 1.6065 - val_accuracy: 0.2568\n",
            "Epoch 44/300\n",
            "459/459 [==============================] - 0s 265us/sample - loss: 1.6019 - accuracy: 0.2702 - val_loss: 1.6063 - val_accuracy: 0.2459\n",
            "Epoch 45/300\n",
            "459/459 [==============================] - 0s 295us/sample - loss: 1.6012 - accuracy: 0.2397 - val_loss: 1.6061 - val_accuracy: 0.2131\n",
            "Epoch 46/300\n",
            "459/459 [==============================] - 0s 312us/sample - loss: 1.6003 - accuracy: 0.2658 - val_loss: 1.6059 - val_accuracy: 0.2568\n",
            "Epoch 47/300\n",
            "459/459 [==============================] - 0s 282us/sample - loss: 1.5995 - accuracy: 0.2963 - val_loss: 1.6056 - val_accuracy: 0.2623\n",
            "Epoch 48/300\n",
            "459/459 [==============================] - 0s 311us/sample - loss: 1.5986 - accuracy: 0.3181 - val_loss: 1.6054 - val_accuracy: 0.2678\n",
            "Epoch 49/300\n",
            "459/459 [==============================] - 0s 262us/sample - loss: 1.5977 - accuracy: 0.3290 - val_loss: 1.6051 - val_accuracy: 0.2787\n",
            "Epoch 50/300\n",
            "459/459 [==============================] - 0s 277us/sample - loss: 1.5968 - accuracy: 0.3442 - val_loss: 1.6047 - val_accuracy: 0.2732\n",
            "Epoch 51/300\n",
            "459/459 [==============================] - 0s 282us/sample - loss: 1.5958 - accuracy: 0.3268 - val_loss: 1.6044 - val_accuracy: 0.2678\n",
            "Epoch 52/300\n",
            "459/459 [==============================] - 0s 284us/sample - loss: 1.5945 - accuracy: 0.3246 - val_loss: 1.6039 - val_accuracy: 0.2623\n",
            "Epoch 53/300\n",
            "459/459 [==============================] - 0s 274us/sample - loss: 1.5932 - accuracy: 0.3377 - val_loss: 1.6035 - val_accuracy: 0.2678\n",
            "Epoch 54/300\n",
            "459/459 [==============================] - 0s 262us/sample - loss: 1.5920 - accuracy: 0.3420 - val_loss: 1.6029 - val_accuracy: 0.2732\n",
            "Epoch 55/300\n",
            "459/459 [==============================] - 0s 275us/sample - loss: 1.5903 - accuracy: 0.3399 - val_loss: 1.6022 - val_accuracy: 0.2678\n",
            "Epoch 56/300\n",
            "459/459 [==============================] - 0s 276us/sample - loss: 1.5886 - accuracy: 0.3420 - val_loss: 1.6017 - val_accuracy: 0.2678\n",
            "Epoch 57/300\n",
            "459/459 [==============================] - 0s 269us/sample - loss: 1.5868 - accuracy: 0.3442 - val_loss: 1.6013 - val_accuracy: 0.2896\n",
            "Epoch 58/300\n",
            "459/459 [==============================] - 0s 330us/sample - loss: 1.5850 - accuracy: 0.3617 - val_loss: 1.6007 - val_accuracy: 0.3060\n",
            "Epoch 59/300\n",
            "459/459 [==============================] - 0s 272us/sample - loss: 1.5830 - accuracy: 0.3725 - val_loss: 1.5997 - val_accuracy: 0.3169\n",
            "Epoch 60/300\n",
            "459/459 [==============================] - 0s 258us/sample - loss: 1.5806 - accuracy: 0.3725 - val_loss: 1.5993 - val_accuracy: 0.2678\n",
            "Epoch 61/300\n",
            "459/459 [==============================] - 0s 284us/sample - loss: 1.5785 - accuracy: 0.3660 - val_loss: 1.5985 - val_accuracy: 0.2732\n",
            "Epoch 62/300\n",
            "459/459 [==============================] - 0s 275us/sample - loss: 1.5763 - accuracy: 0.3551 - val_loss: 1.5979 - val_accuracy: 0.2732\n",
            "Epoch 63/300\n",
            "459/459 [==============================] - 0s 264us/sample - loss: 1.5740 - accuracy: 0.3878 - val_loss: 1.5968 - val_accuracy: 0.2896\n",
            "Epoch 64/300\n",
            "459/459 [==============================] - 0s 289us/sample - loss: 1.5714 - accuracy: 0.3834 - val_loss: 1.5965 - val_accuracy: 0.2787\n",
            "Epoch 65/300\n",
            "459/459 [==============================] - 0s 307us/sample - loss: 1.5688 - accuracy: 0.3900 - val_loss: 1.5954 - val_accuracy: 0.3005\n",
            "Epoch 66/300\n",
            "459/459 [==============================] - 0s 295us/sample - loss: 1.5660 - accuracy: 0.4183 - val_loss: 1.5943 - val_accuracy: 0.2842\n",
            "Epoch 67/300\n",
            "459/459 [==============================] - 0s 302us/sample - loss: 1.5638 - accuracy: 0.4314 - val_loss: 1.5935 - val_accuracy: 0.3060\n",
            "Epoch 68/300\n",
            "459/459 [==============================] - 0s 290us/sample - loss: 1.5605 - accuracy: 0.4336 - val_loss: 1.5931 - val_accuracy: 0.2951\n",
            "Epoch 69/300\n",
            "459/459 [==============================] - 0s 300us/sample - loss: 1.5576 - accuracy: 0.4270 - val_loss: 1.5927 - val_accuracy: 0.2896\n",
            "Epoch 70/300\n",
            "459/459 [==============================] - 0s 265us/sample - loss: 1.5548 - accuracy: 0.4248 - val_loss: 1.5912 - val_accuracy: 0.3115\n",
            "Epoch 71/300\n",
            "459/459 [==============================] - 0s 301us/sample - loss: 1.5517 - accuracy: 0.4183 - val_loss: 1.5906 - val_accuracy: 0.2896\n",
            "Epoch 72/300\n",
            "459/459 [==============================] - 0s 280us/sample - loss: 1.5489 - accuracy: 0.4292 - val_loss: 1.5893 - val_accuracy: 0.2732\n",
            "Epoch 73/300\n",
            "459/459 [==============================] - 0s 278us/sample - loss: 1.5457 - accuracy: 0.4292 - val_loss: 1.5897 - val_accuracy: 0.2842\n",
            "Epoch 74/300\n",
            "459/459 [==============================] - 0s 286us/sample - loss: 1.5418 - accuracy: 0.4488 - val_loss: 1.5875 - val_accuracy: 0.2787\n",
            "Epoch 75/300\n",
            "459/459 [==============================] - 0s 262us/sample - loss: 1.5390 - accuracy: 0.4575 - val_loss: 1.5872 - val_accuracy: 0.3115\n",
            "Epoch 76/300\n",
            "459/459 [==============================] - 0s 295us/sample - loss: 1.5358 - accuracy: 0.4379 - val_loss: 1.5867 - val_accuracy: 0.3005\n",
            "Epoch 77/300\n",
            "459/459 [==============================] - 0s 272us/sample - loss: 1.5320 - accuracy: 0.4379 - val_loss: 1.5855 - val_accuracy: 0.3005\n",
            "Epoch 78/300\n",
            "459/459 [==============================] - 0s 284us/sample - loss: 1.5292 - accuracy: 0.4336 - val_loss: 1.5845 - val_accuracy: 0.3060\n",
            "Epoch 79/300\n",
            "459/459 [==============================] - 0s 244us/sample - loss: 1.5245 - accuracy: 0.4488 - val_loss: 1.5839 - val_accuracy: 0.3060\n",
            "Epoch 80/300\n",
            "459/459 [==============================] - 0s 256us/sample - loss: 1.5228 - accuracy: 0.4793 - val_loss: 1.5822 - val_accuracy: 0.3060\n",
            "Epoch 81/300\n",
            "459/459 [==============================] - 0s 271us/sample - loss: 1.5166 - accuracy: 0.4466 - val_loss: 1.5815 - val_accuracy: 0.3115\n",
            "Epoch 82/300\n",
            "459/459 [==============================] - 0s 272us/sample - loss: 1.5129 - accuracy: 0.4423 - val_loss: 1.5799 - val_accuracy: 0.3169\n",
            "Epoch 83/300\n",
            "459/459 [==============================] - 0s 269us/sample - loss: 1.5108 - accuracy: 0.4488 - val_loss: 1.5795 - val_accuracy: 0.3005\n",
            "Epoch 84/300\n",
            "459/459 [==============================] - 0s 281us/sample - loss: 1.5064 - accuracy: 0.4379 - val_loss: 1.5793 - val_accuracy: 0.3115\n",
            "Epoch 85/300\n",
            "459/459 [==============================] - 0s 275us/sample - loss: 1.5043 - accuracy: 0.4793 - val_loss: 1.5773 - val_accuracy: 0.3224\n",
            "Epoch 86/300\n",
            "459/459 [==============================] - 0s 294us/sample - loss: 1.4978 - accuracy: 0.4466 - val_loss: 1.5763 - val_accuracy: 0.3224\n",
            "Epoch 87/300\n",
            "459/459 [==============================] - 0s 302us/sample - loss: 1.4947 - accuracy: 0.5142 - val_loss: 1.5756 - val_accuracy: 0.3224\n",
            "Epoch 88/300\n",
            "459/459 [==============================] - 0s 288us/sample - loss: 1.4891 - accuracy: 0.4553 - val_loss: 1.5745 - val_accuracy: 0.3169\n",
            "Epoch 89/300\n",
            "459/459 [==============================] - 0s 286us/sample - loss: 1.4844 - accuracy: 0.4553 - val_loss: 1.5733 - val_accuracy: 0.3224\n",
            "Epoch 90/300\n",
            "459/459 [==============================] - 0s 279us/sample - loss: 1.4822 - accuracy: 0.4553 - val_loss: 1.5727 - val_accuracy: 0.3169\n",
            "Epoch 91/300\n",
            "459/459 [==============================] - 0s 297us/sample - loss: 1.4781 - accuracy: 0.5207 - val_loss: 1.5715 - val_accuracy: 0.3279\n",
            "Epoch 92/300\n",
            "459/459 [==============================] - 0s 280us/sample - loss: 1.4714 - accuracy: 0.4662 - val_loss: 1.5717 - val_accuracy: 0.3279\n",
            "Epoch 93/300\n",
            "459/459 [==============================] - 0s 299us/sample - loss: 1.4679 - accuracy: 0.4597 - val_loss: 1.5697 - val_accuracy: 0.3279\n",
            "Epoch 94/300\n",
            "459/459 [==============================] - 0s 276us/sample - loss: 1.4627 - accuracy: 0.4662 - val_loss: 1.5689 - val_accuracy: 0.3279\n",
            "Epoch 95/300\n",
            "459/459 [==============================] - 0s 275us/sample - loss: 1.4582 - accuracy: 0.5403 - val_loss: 1.5675 - val_accuracy: 0.3388\n",
            "Epoch 96/300\n",
            "459/459 [==============================] - 0s 297us/sample - loss: 1.4552 - accuracy: 0.4662 - val_loss: 1.5675 - val_accuracy: 0.3388\n",
            "Epoch 97/300\n",
            "459/459 [==============================] - 0s 287us/sample - loss: 1.4486 - accuracy: 0.5163 - val_loss: 1.5656 - val_accuracy: 0.3552\n",
            "Epoch 98/300\n",
            "459/459 [==============================] - 0s 295us/sample - loss: 1.4434 - accuracy: 0.5251 - val_loss: 1.5653 - val_accuracy: 0.3607\n",
            "Epoch 99/300\n",
            "459/459 [==============================] - 0s 302us/sample - loss: 1.4382 - accuracy: 0.5316 - val_loss: 1.5637 - val_accuracy: 0.3607\n",
            "Epoch 100/300\n",
            "459/459 [==============================] - 0s 290us/sample - loss: 1.4328 - accuracy: 0.5338 - val_loss: 1.5623 - val_accuracy: 0.3661\n",
            "Epoch 101/300\n",
            "459/459 [==============================] - 0s 284us/sample - loss: 1.4286 - accuracy: 0.5272 - val_loss: 1.5615 - val_accuracy: 0.3661\n",
            "Epoch 102/300\n",
            "459/459 [==============================] - 0s 296us/sample - loss: 1.4232 - accuracy: 0.5316 - val_loss: 1.5602 - val_accuracy: 0.3607\n",
            "Epoch 103/300\n",
            "459/459 [==============================] - 0s 296us/sample - loss: 1.4184 - accuracy: 0.5708 - val_loss: 1.5592 - val_accuracy: 0.3607\n",
            "Epoch 104/300\n",
            "459/459 [==============================] - 0s 290us/sample - loss: 1.4128 - accuracy: 0.4880 - val_loss: 1.5591 - val_accuracy: 0.3497\n",
            "Epoch 105/300\n",
            "459/459 [==============================] - 0s 301us/sample - loss: 1.4074 - accuracy: 0.5490 - val_loss: 1.5573 - val_accuracy: 0.3443\n",
            "Epoch 106/300\n",
            "459/459 [==============================] - 0s 286us/sample - loss: 1.4032 - accuracy: 0.5708 - val_loss: 1.5564 - val_accuracy: 0.3661\n",
            "Epoch 107/300\n",
            "459/459 [==============================] - 0s 290us/sample - loss: 1.3976 - accuracy: 0.5773 - val_loss: 1.5554 - val_accuracy: 0.3443\n",
            "Epoch 108/300\n",
            "459/459 [==============================] - 0s 302us/sample - loss: 1.3925 - accuracy: 0.5447 - val_loss: 1.5547 - val_accuracy: 0.3661\n",
            "Epoch 109/300\n",
            "459/459 [==============================] - 0s 309us/sample - loss: 1.3877 - accuracy: 0.5861 - val_loss: 1.5539 - val_accuracy: 0.3497\n",
            "Epoch 110/300\n",
            "459/459 [==============================] - 0s 338us/sample - loss: 1.3827 - accuracy: 0.5948 - val_loss: 1.5530 - val_accuracy: 0.3880\n",
            "Epoch 111/300\n",
            "459/459 [==============================] - 0s 287us/sample - loss: 1.3770 - accuracy: 0.5447 - val_loss: 1.5525 - val_accuracy: 0.3607\n",
            "Epoch 112/300\n",
            "459/459 [==============================] - 0s 315us/sample - loss: 1.3733 - accuracy: 0.4728 - val_loss: 1.5527 - val_accuracy: 0.3443\n",
            "Epoch 113/300\n",
            "459/459 [==============================] - 0s 296us/sample - loss: 1.3675 - accuracy: 0.5425 - val_loss: 1.5504 - val_accuracy: 0.3552\n",
            "Epoch 114/300\n",
            "459/459 [==============================] - 0s 338us/sample - loss: 1.3620 - accuracy: 0.6013 - val_loss: 1.5500 - val_accuracy: 0.3661\n",
            "Epoch 115/300\n",
            "459/459 [==============================] - 0s 296us/sample - loss: 1.3565 - accuracy: 0.5686 - val_loss: 1.5505 - val_accuracy: 0.3607\n",
            "Epoch 116/300\n",
            "459/459 [==============================] - 0s 271us/sample - loss: 1.3512 - accuracy: 0.5643 - val_loss: 1.5495 - val_accuracy: 0.3607\n",
            "Epoch 117/300\n",
            "459/459 [==============================] - 0s 284us/sample - loss: 1.3460 - accuracy: 0.5556 - val_loss: 1.5496 - val_accuracy: 0.3607\n",
            "Epoch 118/300\n",
            "459/459 [==============================] - 0s 310us/sample - loss: 1.3406 - accuracy: 0.5795 - val_loss: 1.5488 - val_accuracy: 0.3443\n",
            "Epoch 119/300\n",
            "459/459 [==============================] - 0s 291us/sample - loss: 1.3355 - accuracy: 0.6057 - val_loss: 1.5485 - val_accuracy: 0.3497\n",
            "Epoch 120/300\n",
            "459/459 [==============================] - 0s 263us/sample - loss: 1.3306 - accuracy: 0.6144 - val_loss: 1.5486 - val_accuracy: 0.3552\n",
            "Epoch 121/300\n",
            "459/459 [==============================] - 0s 290us/sample - loss: 1.3256 - accuracy: 0.5926 - val_loss: 1.5485 - val_accuracy: 0.3497\n",
            "Epoch 122/300\n",
            "459/459 [==============================] - 0s 283us/sample - loss: 1.3204 - accuracy: 0.5969 - val_loss: 1.5479 - val_accuracy: 0.3497\n",
            "Epoch 123/300\n",
            "459/459 [==============================] - 0s 296us/sample - loss: 1.3154 - accuracy: 0.5991 - val_loss: 1.5475 - val_accuracy: 0.3443\n",
            "Epoch 124/300\n",
            "459/459 [==============================] - 0s 266us/sample - loss: 1.3113 - accuracy: 0.5490 - val_loss: 1.5481 - val_accuracy: 0.3661\n",
            "Epoch 125/300\n",
            "459/459 [==============================] - 0s 284us/sample - loss: 1.3052 - accuracy: 0.5773 - val_loss: 1.5472 - val_accuracy: 0.3497\n",
            "Epoch 126/300\n",
            "459/459 [==============================] - 0s 335us/sample - loss: 1.3006 - accuracy: 0.5882 - val_loss: 1.5470 - val_accuracy: 0.3443\n",
            "Epoch 127/300\n",
            "459/459 [==============================] - 0s 291us/sample - loss: 1.2958 - accuracy: 0.6078 - val_loss: 1.5469 - val_accuracy: 0.3388\n",
            "Epoch 128/300\n",
            "459/459 [==============================] - 0s 289us/sample - loss: 1.2902 - accuracy: 0.6078 - val_loss: 1.5469 - val_accuracy: 0.3497\n",
            "Epoch 129/300\n",
            "459/459 [==============================] - 0s 276us/sample - loss: 1.2855 - accuracy: 0.6035 - val_loss: 1.5466 - val_accuracy: 0.3552\n",
            "Epoch 130/300\n",
            "459/459 [==============================] - 0s 296us/sample - loss: 1.2806 - accuracy: 0.5490 - val_loss: 1.5469 - val_accuracy: 0.3443\n",
            "Epoch 131/300\n",
            "459/459 [==============================] - 0s 266us/sample - loss: 1.2763 - accuracy: 0.5425 - val_loss: 1.5465 - val_accuracy: 0.3224\n",
            "Epoch 132/300\n",
            "459/459 [==============================] - 0s 272us/sample - loss: 1.2712 - accuracy: 0.5817 - val_loss: 1.5469 - val_accuracy: 0.3607\n",
            "Epoch 133/300\n",
            "459/459 [==============================] - 0s 290us/sample - loss: 1.2667 - accuracy: 0.5425 - val_loss: 1.5471 - val_accuracy: 0.3607\n",
            "Epoch 134/300\n",
            "459/459 [==============================] - 0s 285us/sample - loss: 1.2621 - accuracy: 0.5991 - val_loss: 1.5467 - val_accuracy: 0.3388\n",
            "Epoch 135/300\n",
            "459/459 [==============================] - 0s 295us/sample - loss: 1.2570 - accuracy: 0.6078 - val_loss: 1.5479 - val_accuracy: 0.3497\n",
            "Epoch 136/300\n",
            "459/459 [==============================] - 0s 296us/sample - loss: 1.2523 - accuracy: 0.6035 - val_loss: 1.5489 - val_accuracy: 0.3497\n",
            "Epoch 137/300\n",
            "459/459 [==============================] - 0s 315us/sample - loss: 1.2478 - accuracy: 0.6057 - val_loss: 1.5493 - val_accuracy: 0.3497\n",
            "Epoch 138/300\n",
            "459/459 [==============================] - 0s 282us/sample - loss: 1.2428 - accuracy: 0.6100 - val_loss: 1.5491 - val_accuracy: 0.3443\n",
            "Epoch 139/300\n",
            "459/459 [==============================] - 0s 266us/sample - loss: 1.2384 - accuracy: 0.6100 - val_loss: 1.5497 - val_accuracy: 0.3443\n",
            "Epoch 140/300\n",
            "459/459 [==============================] - 0s 265us/sample - loss: 1.2341 - accuracy: 0.6057 - val_loss: 1.5503 - val_accuracy: 0.3497\n",
            "Epoch 141/300\n",
            "459/459 [==============================] - 0s 290us/sample - loss: 1.2296 - accuracy: 0.6100 - val_loss: 1.5499 - val_accuracy: 0.3497\n",
            "Epoch 142/300\n",
            "459/459 [==============================] - 0s 287us/sample - loss: 1.2244 - accuracy: 0.6144 - val_loss: 1.5505 - val_accuracy: 0.3443\n",
            "Epoch 143/300\n",
            "459/459 [==============================] - 0s 302us/sample - loss: 1.2197 - accuracy: 0.6078 - val_loss: 1.5510 - val_accuracy: 0.3388\n",
            "Epoch 144/300\n",
            "459/459 [==============================] - 0s 308us/sample - loss: 1.2158 - accuracy: 0.6100 - val_loss: 1.5513 - val_accuracy: 0.3443\n",
            "Epoch 145/300\n",
            "459/459 [==============================] - 0s 276us/sample - loss: 1.2105 - accuracy: 0.6144 - val_loss: 1.5514 - val_accuracy: 0.3497\n",
            "Epoch 146/300\n",
            "459/459 [==============================] - 0s 275us/sample - loss: 1.2061 - accuracy: 0.6100 - val_loss: 1.5520 - val_accuracy: 0.3497\n",
            "Epoch 147/300\n",
            "459/459 [==============================] - 0s 270us/sample - loss: 1.2016 - accuracy: 0.6100 - val_loss: 1.5522 - val_accuracy: 0.3333\n",
            "Epoch 148/300\n",
            "459/459 [==============================] - 0s 267us/sample - loss: 1.1973 - accuracy: 0.6078 - val_loss: 1.5528 - val_accuracy: 0.3443\n",
            "Epoch 149/300\n",
            "459/459 [==============================] - 0s 287us/sample - loss: 1.1926 - accuracy: 0.6144 - val_loss: 1.5536 - val_accuracy: 0.3333\n",
            "Epoch 150/300\n",
            "459/459 [==============================] - 0s 294us/sample - loss: 1.1878 - accuracy: 0.6100 - val_loss: 1.5548 - val_accuracy: 0.3388\n",
            "Epoch 151/300\n",
            "459/459 [==============================] - 0s 319us/sample - loss: 1.1834 - accuracy: 0.6078 - val_loss: 1.5559 - val_accuracy: 0.3388\n",
            "Epoch 152/300\n",
            "459/459 [==============================] - 0s 295us/sample - loss: 1.1787 - accuracy: 0.6100 - val_loss: 1.5563 - val_accuracy: 0.3224\n",
            "Epoch 153/300\n",
            "459/459 [==============================] - 0s 264us/sample - loss: 1.1744 - accuracy: 0.6013 - val_loss: 1.5576 - val_accuracy: 0.3333\n",
            "Epoch 154/300\n",
            "459/459 [==============================] - 0s 258us/sample - loss: 1.1696 - accuracy: 0.6100 - val_loss: 1.5589 - val_accuracy: 0.3169\n",
            "Epoch 155/300\n",
            "459/459 [==============================] - 0s 277us/sample - loss: 1.1666 - accuracy: 0.6078 - val_loss: 1.5603 - val_accuracy: 0.3115\n",
            "Epoch 156/300\n",
            "459/459 [==============================] - 0s 260us/sample - loss: 1.1608 - accuracy: 0.6100 - val_loss: 1.5601 - val_accuracy: 0.3169\n",
            "Epoch 157/300\n",
            "459/459 [==============================] - 0s 261us/sample - loss: 1.1567 - accuracy: 0.6057 - val_loss: 1.5611 - val_accuracy: 0.3115\n",
            "Epoch 158/300\n",
            "459/459 [==============================] - 0s 250us/sample - loss: 1.1525 - accuracy: 0.6144 - val_loss: 1.5619 - val_accuracy: 0.3279\n",
            "Epoch 159/300\n",
            "459/459 [==============================] - 0s 275us/sample - loss: 1.1484 - accuracy: 0.6122 - val_loss: 1.5628 - val_accuracy: 0.3224\n",
            "Epoch 160/300\n",
            "459/459 [==============================] - 0s 296us/sample - loss: 1.1438 - accuracy: 0.6122 - val_loss: 1.5635 - val_accuracy: 0.3279\n",
            "Epoch 161/300\n",
            "459/459 [==============================] - 0s 276us/sample - loss: 1.1399 - accuracy: 0.6144 - val_loss: 1.5647 - val_accuracy: 0.3279\n",
            "Epoch 162/300\n",
            "459/459 [==============================] - 0s 273us/sample - loss: 1.1359 - accuracy: 0.6144 - val_loss: 1.5657 - val_accuracy: 0.3115\n",
            "Epoch 163/300\n",
            "459/459 [==============================] - 0s 274us/sample - loss: 1.1319 - accuracy: 0.6100 - val_loss: 1.5675 - val_accuracy: 0.3443\n",
            "Epoch 164/300\n",
            "459/459 [==============================] - 0s 280us/sample - loss: 1.1273 - accuracy: 0.6144 - val_loss: 1.5683 - val_accuracy: 0.3388\n",
            "Epoch 165/300\n",
            "459/459 [==============================] - 0s 305us/sample - loss: 1.1236 - accuracy: 0.6122 - val_loss: 1.5686 - val_accuracy: 0.3279\n",
            "Epoch 166/300\n",
            "459/459 [==============================] - 0s 284us/sample - loss: 1.1190 - accuracy: 0.6166 - val_loss: 1.5695 - val_accuracy: 0.3224\n",
            "Epoch 167/300\n",
            "459/459 [==============================] - 0s 258us/sample - loss: 1.1165 - accuracy: 0.6122 - val_loss: 1.5712 - val_accuracy: 0.3224\n",
            "Epoch 168/300\n",
            "459/459 [==============================] - 0s 304us/sample - loss: 1.1113 - accuracy: 0.6144 - val_loss: 1.5710 - val_accuracy: 0.3224\n",
            "Epoch 169/300\n",
            "459/459 [==============================] - 0s 255us/sample - loss: 1.1077 - accuracy: 0.6100 - val_loss: 1.5719 - val_accuracy: 0.3060\n",
            "Epoch 170/300\n",
            "459/459 [==============================] - 0s 271us/sample - loss: 1.1042 - accuracy: 0.6100 - val_loss: 1.5724 - val_accuracy: 0.3279\n",
            "Epoch 171/300\n",
            "459/459 [==============================] - 0s 279us/sample - loss: 1.0998 - accuracy: 0.6100 - val_loss: 1.5732 - val_accuracy: 0.3279\n",
            "Epoch 172/300\n",
            "459/459 [==============================] - 0s 281us/sample - loss: 1.0965 - accuracy: 0.6122 - val_loss: 1.5734 - val_accuracy: 0.3224\n",
            "Epoch 173/300\n",
            "459/459 [==============================] - 0s 263us/sample - loss: 1.0924 - accuracy: 0.6166 - val_loss: 1.5741 - val_accuracy: 0.3279\n",
            "Epoch 174/300\n",
            "459/459 [==============================] - 0s 279us/sample - loss: 1.0889 - accuracy: 0.6144 - val_loss: 1.5746 - val_accuracy: 0.3060\n",
            "Epoch 175/300\n",
            "459/459 [==============================] - 0s 271us/sample - loss: 1.0836 - accuracy: 0.6144 - val_loss: 1.5757 - val_accuracy: 0.3169\n",
            "Epoch 176/300\n",
            "459/459 [==============================] - 0s 285us/sample - loss: 1.0810 - accuracy: 0.6100 - val_loss: 1.5767 - val_accuracy: 0.3169\n",
            "Epoch 177/300\n",
            "459/459 [==============================] - 0s 265us/sample - loss: 1.0764 - accuracy: 0.6166 - val_loss: 1.5774 - val_accuracy: 0.3224\n",
            "Epoch 178/300\n",
            "459/459 [==============================] - 0s 269us/sample - loss: 1.0726 - accuracy: 0.6144 - val_loss: 1.5783 - val_accuracy: 0.3060\n",
            "Epoch 179/300\n",
            "459/459 [==============================] - 0s 268us/sample - loss: 1.0693 - accuracy: 0.6100 - val_loss: 1.5787 - val_accuracy: 0.3060\n",
            "Epoch 180/300\n",
            "459/459 [==============================] - 0s 283us/sample - loss: 1.0656 - accuracy: 0.6100 - val_loss: 1.5804 - val_accuracy: 0.3060\n",
            "Epoch 181/300\n",
            "459/459 [==============================] - 0s 268us/sample - loss: 1.0624 - accuracy: 0.6122 - val_loss: 1.5818 - val_accuracy: 0.3169\n",
            "Epoch 182/300\n",
            "459/459 [==============================] - 0s 286us/sample - loss: 1.0589 - accuracy: 0.6166 - val_loss: 1.5828 - val_accuracy: 0.3224\n",
            "Epoch 183/300\n",
            "459/459 [==============================] - 0s 296us/sample - loss: 1.0551 - accuracy: 0.6144 - val_loss: 1.5835 - val_accuracy: 0.3169\n",
            "Epoch 184/300\n",
            "459/459 [==============================] - 0s 280us/sample - loss: 1.0523 - accuracy: 0.6122 - val_loss: 1.5851 - val_accuracy: 0.3169\n",
            "Epoch 185/300\n",
            "459/459 [==============================] - 0s 269us/sample - loss: 1.0492 - accuracy: 0.6166 - val_loss: 1.5864 - val_accuracy: 0.3115\n",
            "Epoch 186/300\n",
            "459/459 [==============================] - 0s 313us/sample - loss: 1.0449 - accuracy: 0.6122 - val_loss: 1.5875 - val_accuracy: 0.3169\n",
            "Epoch 187/300\n",
            "459/459 [==============================] - 0s 301us/sample - loss: 1.0417 - accuracy: 0.6187 - val_loss: 1.5871 - val_accuracy: 0.3115\n",
            "Epoch 188/300\n",
            "459/459 [==============================] - 0s 281us/sample - loss: 1.0383 - accuracy: 0.6122 - val_loss: 1.5888 - val_accuracy: 0.3060\n",
            "Epoch 189/300\n",
            "459/459 [==============================] - 0s 290us/sample - loss: 1.0350 - accuracy: 0.6144 - val_loss: 1.5894 - val_accuracy: 0.3169\n",
            "Epoch 190/300\n",
            "459/459 [==============================] - 0s 270us/sample - loss: 1.0321 - accuracy: 0.6144 - val_loss: 1.5906 - val_accuracy: 0.3169\n",
            "Epoch 191/300\n",
            "459/459 [==============================] - 0s 292us/sample - loss: 1.0289 - accuracy: 0.6166 - val_loss: 1.5921 - val_accuracy: 0.3115\n",
            "Epoch 192/300\n",
            "459/459 [==============================] - 0s 319us/sample - loss: 1.0254 - accuracy: 0.6166 - val_loss: 1.5934 - val_accuracy: 0.3115\n",
            "Epoch 193/300\n",
            "459/459 [==============================] - 0s 279us/sample - loss: 1.0224 - accuracy: 0.6166 - val_loss: 1.5951 - val_accuracy: 0.3115\n",
            "Epoch 194/300\n",
            "459/459 [==============================] - 0s 275us/sample - loss: 1.0191 - accuracy: 0.6166 - val_loss: 1.5965 - val_accuracy: 0.3115\n",
            "Epoch 195/300\n",
            "459/459 [==============================] - 0s 268us/sample - loss: 1.0161 - accuracy: 0.6166 - val_loss: 1.5973 - val_accuracy: 0.3115\n",
            "Epoch 196/300\n",
            "459/459 [==============================] - 0s 274us/sample - loss: 1.0138 - accuracy: 0.6144 - val_loss: 1.5982 - val_accuracy: 0.3115\n",
            "Epoch 197/300\n",
            "459/459 [==============================] - 0s 284us/sample - loss: 1.0098 - accuracy: 0.6144 - val_loss: 1.5995 - val_accuracy: 0.3115\n",
            "Epoch 198/300\n",
            "459/459 [==============================] - 0s 345us/sample - loss: 1.0073 - accuracy: 0.6187 - val_loss: 1.6006 - val_accuracy: 0.3169\n",
            "Epoch 199/300\n",
            "459/459 [==============================] - 0s 298us/sample - loss: 1.0038 - accuracy: 0.6187 - val_loss: 1.6017 - val_accuracy: 0.3115\n",
            "Epoch 200/300\n",
            "459/459 [==============================] - 0s 274us/sample - loss: 1.0009 - accuracy: 0.6187 - val_loss: 1.6031 - val_accuracy: 0.3115\n",
            "Epoch 201/300\n",
            "459/459 [==============================] - 0s 269us/sample - loss: 0.9982 - accuracy: 0.6187 - val_loss: 1.6037 - val_accuracy: 0.3115\n",
            "Epoch 202/300\n",
            "459/459 [==============================] - 0s 268us/sample - loss: 0.9948 - accuracy: 0.6209 - val_loss: 1.6051 - val_accuracy: 0.3169\n",
            "Epoch 203/300\n",
            "459/459 [==============================] - 0s 258us/sample - loss: 0.9920 - accuracy: 0.6144 - val_loss: 1.6067 - val_accuracy: 0.3224\n",
            "Epoch 204/300\n",
            "459/459 [==============================] - 0s 262us/sample - loss: 0.9890 - accuracy: 0.6122 - val_loss: 1.6082 - val_accuracy: 0.3224\n",
            "Epoch 205/300\n",
            "459/459 [==============================] - 0s 287us/sample - loss: 0.9863 - accuracy: 0.6144 - val_loss: 1.6101 - val_accuracy: 0.3224\n",
            "Epoch 206/300\n",
            "459/459 [==============================] - 0s 272us/sample - loss: 0.9829 - accuracy: 0.6166 - val_loss: 1.6115 - val_accuracy: 0.3224\n",
            "Epoch 207/300\n",
            "459/459 [==============================] - 0s 296us/sample - loss: 0.9801 - accuracy: 0.6144 - val_loss: 1.6123 - val_accuracy: 0.3169\n",
            "Epoch 208/300\n",
            "459/459 [==============================] - 0s 302us/sample - loss: 0.9774 - accuracy: 0.6209 - val_loss: 1.6153 - val_accuracy: 0.3224\n",
            "Epoch 209/300\n",
            "459/459 [==============================] - 0s 289us/sample - loss: 0.9747 - accuracy: 0.5948 - val_loss: 1.6167 - val_accuracy: 0.3224\n",
            "Epoch 210/300\n",
            "459/459 [==============================] - 0s 287us/sample - loss: 0.9718 - accuracy: 0.6209 - val_loss: 1.6175 - val_accuracy: 0.3169\n",
            "Epoch 211/300\n",
            "459/459 [==============================] - 0s 298us/sample - loss: 0.9695 - accuracy: 0.6100 - val_loss: 1.6191 - val_accuracy: 0.3224\n",
            "Epoch 212/300\n",
            "459/459 [==============================] - 0s 297us/sample - loss: 0.9664 - accuracy: 0.6078 - val_loss: 1.6200 - val_accuracy: 0.3224\n",
            "Epoch 213/300\n",
            "459/459 [==============================] - 0s 280us/sample - loss: 0.9643 - accuracy: 0.5882 - val_loss: 1.6222 - val_accuracy: 0.3279\n",
            "Epoch 214/300\n",
            "459/459 [==============================] - 0s 316us/sample - loss: 0.9611 - accuracy: 0.6057 - val_loss: 1.6245 - val_accuracy: 0.3169\n",
            "Epoch 215/300\n",
            "459/459 [==============================] - 0s 324us/sample - loss: 0.9587 - accuracy: 0.6187 - val_loss: 1.6253 - val_accuracy: 0.3169\n",
            "Epoch 216/300\n",
            "459/459 [==============================] - 0s 296us/sample - loss: 0.9562 - accuracy: 0.6209 - val_loss: 1.6269 - val_accuracy: 0.3005\n",
            "Epoch 217/300\n",
            "459/459 [==============================] - 0s 316us/sample - loss: 0.9537 - accuracy: 0.6231 - val_loss: 1.6294 - val_accuracy: 0.3169\n",
            "Epoch 218/300\n",
            "459/459 [==============================] - 0s 305us/sample - loss: 0.9512 - accuracy: 0.6209 - val_loss: 1.6309 - val_accuracy: 0.3169\n",
            "Epoch 219/300\n",
            "459/459 [==============================] - 0s 303us/sample - loss: 0.9488 - accuracy: 0.6209 - val_loss: 1.6330 - val_accuracy: 0.3169\n",
            "Epoch 220/300\n",
            "459/459 [==============================] - 0s 292us/sample - loss: 0.9464 - accuracy: 0.6209 - val_loss: 1.6341 - val_accuracy: 0.3169\n",
            "Epoch 221/300\n",
            "459/459 [==============================] - 0s 262us/sample - loss: 0.9439 - accuracy: 0.6231 - val_loss: 1.6363 - val_accuracy: 0.3169\n",
            "Epoch 222/300\n",
            "459/459 [==============================] - 0s 323us/sample - loss: 0.9417 - accuracy: 0.6231 - val_loss: 1.6374 - val_accuracy: 0.3005\n",
            "Epoch 223/300\n",
            "459/459 [==============================] - 0s 283us/sample - loss: 0.9392 - accuracy: 0.6166 - val_loss: 1.6392 - val_accuracy: 0.3115\n",
            "Epoch 224/300\n",
            "459/459 [==============================] - 0s 278us/sample - loss: 0.9369 - accuracy: 0.6231 - val_loss: 1.6402 - val_accuracy: 0.3005\n",
            "Epoch 225/300\n",
            "459/459 [==============================] - 0s 276us/sample - loss: 0.9345 - accuracy: 0.6209 - val_loss: 1.6419 - val_accuracy: 0.2896\n",
            "Epoch 226/300\n",
            "459/459 [==============================] - 0s 280us/sample - loss: 0.9326 - accuracy: 0.6166 - val_loss: 1.6434 - val_accuracy: 0.2951\n",
            "Epoch 227/300\n",
            "459/459 [==============================] - 0s 274us/sample - loss: 0.9299 - accuracy: 0.6187 - val_loss: 1.6446 - val_accuracy: 0.3005\n",
            "Epoch 228/300\n",
            "459/459 [==============================] - 0s 286us/sample - loss: 0.9280 - accuracy: 0.6231 - val_loss: 1.6466 - val_accuracy: 0.3060\n",
            "Epoch 229/300\n",
            "459/459 [==============================] - 0s 287us/sample - loss: 0.9256 - accuracy: 0.6253 - val_loss: 1.6504 - val_accuracy: 0.3115\n",
            "Epoch 230/300\n",
            "459/459 [==============================] - 0s 296us/sample - loss: 0.9234 - accuracy: 0.6209 - val_loss: 1.6529 - val_accuracy: 0.3115\n",
            "Epoch 231/300\n",
            "459/459 [==============================] - 0s 307us/sample - loss: 0.9211 - accuracy: 0.6231 - val_loss: 1.6530 - val_accuracy: 0.3060\n",
            "Epoch 232/300\n",
            "459/459 [==============================] - 0s 284us/sample - loss: 0.9189 - accuracy: 0.6275 - val_loss: 1.6549 - val_accuracy: 0.3005\n",
            "Epoch 233/300\n",
            "459/459 [==============================] - 0s 273us/sample - loss: 0.9167 - accuracy: 0.6253 - val_loss: 1.6553 - val_accuracy: 0.3005\n",
            "Epoch 234/300\n",
            "459/459 [==============================] - 0s 285us/sample - loss: 0.9144 - accuracy: 0.6275 - val_loss: 1.6569 - val_accuracy: 0.3115\n",
            "Epoch 235/300\n",
            "459/459 [==============================] - 0s 309us/sample - loss: 0.9122 - accuracy: 0.6166 - val_loss: 1.6591 - val_accuracy: 0.3115\n",
            "Epoch 236/300\n",
            "459/459 [==============================] - 0s 279us/sample - loss: 0.9102 - accuracy: 0.6144 - val_loss: 1.6608 - val_accuracy: 0.3115\n",
            "Epoch 237/300\n",
            "459/459 [==============================] - 0s 280us/sample - loss: 0.9085 - accuracy: 0.6253 - val_loss: 1.6621 - val_accuracy: 0.3060\n",
            "Epoch 238/300\n",
            "459/459 [==============================] - 0s 268us/sample - loss: 0.9059 - accuracy: 0.6253 - val_loss: 1.6649 - val_accuracy: 0.3115\n",
            "Epoch 239/300\n",
            "459/459 [==============================] - 0s 259us/sample - loss: 0.9041 - accuracy: 0.6231 - val_loss: 1.6672 - val_accuracy: 0.3115\n",
            "Epoch 240/300\n",
            "459/459 [==============================] - 0s 289us/sample - loss: 0.9019 - accuracy: 0.6231 - val_loss: 1.6683 - val_accuracy: 0.3115\n",
            "Epoch 241/300\n",
            "459/459 [==============================] - 0s 269us/sample - loss: 0.8999 - accuracy: 0.6209 - val_loss: 1.6688 - val_accuracy: 0.3115\n",
            "Epoch 242/300\n",
            "459/459 [==============================] - 0s 266us/sample - loss: 0.8976 - accuracy: 0.6231 - val_loss: 1.6710 - val_accuracy: 0.3060\n",
            "Epoch 243/300\n",
            "459/459 [==============================] - 0s 266us/sample - loss: 0.8958 - accuracy: 0.6231 - val_loss: 1.6717 - val_accuracy: 0.3060\n",
            "Epoch 244/300\n",
            "459/459 [==============================] - 0s 282us/sample - loss: 0.8938 - accuracy: 0.6253 - val_loss: 1.6742 - val_accuracy: 0.3115\n",
            "Epoch 245/300\n",
            "459/459 [==============================] - 0s 326us/sample - loss: 0.8918 - accuracy: 0.6231 - val_loss: 1.6757 - val_accuracy: 0.3115\n",
            "Epoch 246/300\n",
            "459/459 [==============================] - 0s 298us/sample - loss: 0.8898 - accuracy: 0.6253 - val_loss: 1.6771 - val_accuracy: 0.3060\n",
            "Epoch 247/300\n",
            "459/459 [==============================] - 0s 328us/sample - loss: 0.8885 - accuracy: 0.6253 - val_loss: 1.6803 - val_accuracy: 0.3115\n",
            "Epoch 248/300\n",
            "459/459 [==============================] - 0s 300us/sample - loss: 0.8862 - accuracy: 0.6253 - val_loss: 1.6840 - val_accuracy: 0.3060\n",
            "Epoch 249/300\n",
            "459/459 [==============================] - 0s 286us/sample - loss: 0.8842 - accuracy: 0.6231 - val_loss: 1.6853 - val_accuracy: 0.3060\n",
            "Epoch 250/300\n",
            "459/459 [==============================] - 0s 297us/sample - loss: 0.8823 - accuracy: 0.6209 - val_loss: 1.6875 - val_accuracy: 0.3060\n",
            "Epoch 251/300\n",
            "459/459 [==============================] - 0s 283us/sample - loss: 0.8806 - accuracy: 0.5969 - val_loss: 1.6884 - val_accuracy: 0.3005\n",
            "Epoch 252/300\n",
            "459/459 [==============================] - 0s 300us/sample - loss: 0.8788 - accuracy: 0.6013 - val_loss: 1.6872 - val_accuracy: 0.3060\n",
            "Epoch 253/300\n",
            "459/459 [==============================] - 0s 270us/sample - loss: 0.8771 - accuracy: 0.6166 - val_loss: 1.6888 - val_accuracy: 0.3060\n",
            "Epoch 254/300\n",
            "459/459 [==============================] - 0s 293us/sample - loss: 0.8753 - accuracy: 0.6231 - val_loss: 1.6944 - val_accuracy: 0.3060\n",
            "Epoch 255/300\n",
            "459/459 [==============================] - 0s 317us/sample - loss: 0.8735 - accuracy: 0.6209 - val_loss: 1.6977 - val_accuracy: 0.3060\n",
            "Epoch 256/300\n",
            "459/459 [==============================] - 0s 312us/sample - loss: 0.8717 - accuracy: 0.6231 - val_loss: 1.7000 - val_accuracy: 0.3005\n",
            "Epoch 257/300\n",
            "459/459 [==============================] - 0s 308us/sample - loss: 0.8700 - accuracy: 0.6187 - val_loss: 1.6999 - val_accuracy: 0.3060\n",
            "Epoch 258/300\n",
            "459/459 [==============================] - 0s 268us/sample - loss: 0.8683 - accuracy: 0.6231 - val_loss: 1.7034 - val_accuracy: 0.3060\n",
            "Epoch 259/300\n",
            "459/459 [==============================] - 0s 311us/sample - loss: 0.8667 - accuracy: 0.6296 - val_loss: 1.7042 - val_accuracy: 0.3060\n",
            "Epoch 260/300\n",
            "459/459 [==============================] - 0s 289us/sample - loss: 0.8650 - accuracy: 0.6296 - val_loss: 1.7070 - val_accuracy: 0.3005\n",
            "Epoch 261/300\n",
            "459/459 [==============================] - 0s 273us/sample - loss: 0.8634 - accuracy: 0.6275 - val_loss: 1.7097 - val_accuracy: 0.3005\n",
            "Epoch 262/300\n",
            "459/459 [==============================] - 0s 345us/sample - loss: 0.8616 - accuracy: 0.6253 - val_loss: 1.7117 - val_accuracy: 0.3005\n",
            "Epoch 263/300\n",
            "459/459 [==============================] - 0s 280us/sample - loss: 0.8600 - accuracy: 0.6296 - val_loss: 1.7147 - val_accuracy: 0.3060\n",
            "Epoch 264/300\n",
            "459/459 [==============================] - 0s 297us/sample - loss: 0.8584 - accuracy: 0.6275 - val_loss: 1.7152 - val_accuracy: 0.3005\n",
            "Epoch 265/300\n",
            "459/459 [==============================] - 0s 274us/sample - loss: 0.8569 - accuracy: 0.6275 - val_loss: 1.7147 - val_accuracy: 0.3005\n",
            "Epoch 266/300\n",
            "459/459 [==============================] - 0s 267us/sample - loss: 0.8558 - accuracy: 0.6231 - val_loss: 1.7197 - val_accuracy: 0.3060\n",
            "Epoch 267/300\n",
            "459/459 [==============================] - 0s 295us/sample - loss: 0.8536 - accuracy: 0.6231 - val_loss: 1.7182 - val_accuracy: 0.3060\n",
            "Epoch 268/300\n",
            "459/459 [==============================] - 0s 295us/sample - loss: 0.8520 - accuracy: 0.6275 - val_loss: 1.7189 - val_accuracy: 0.3115\n",
            "Epoch 269/300\n",
            "459/459 [==============================] - 0s 324us/sample - loss: 0.8507 - accuracy: 0.6340 - val_loss: 1.7241 - val_accuracy: 0.3060\n",
            "Epoch 270/300\n",
            "459/459 [==============================] - 0s 285us/sample - loss: 0.8490 - accuracy: 0.6296 - val_loss: 1.7276 - val_accuracy: 0.3060\n",
            "Epoch 271/300\n",
            "459/459 [==============================] - 0s 315us/sample - loss: 0.8476 - accuracy: 0.6340 - val_loss: 1.7301 - val_accuracy: 0.3060\n",
            "Epoch 272/300\n",
            "459/459 [==============================] - 0s 279us/sample - loss: 0.8459 - accuracy: 0.6253 - val_loss: 1.7322 - val_accuracy: 0.3060\n",
            "Epoch 273/300\n",
            "459/459 [==============================] - 0s 279us/sample - loss: 0.8445 - accuracy: 0.6253 - val_loss: 1.7344 - val_accuracy: 0.3060\n",
            "Epoch 274/300\n",
            "459/459 [==============================] - 0s 304us/sample - loss: 0.8429 - accuracy: 0.6253 - val_loss: 1.7363 - val_accuracy: 0.3060\n",
            "Epoch 275/300\n",
            "459/459 [==============================] - 0s 286us/sample - loss: 0.8415 - accuracy: 0.6275 - val_loss: 1.7389 - val_accuracy: 0.3060\n",
            "Epoch 276/300\n",
            "459/459 [==============================] - 0s 293us/sample - loss: 0.8398 - accuracy: 0.6253 - val_loss: 1.7432 - val_accuracy: 0.2951\n",
            "Epoch 277/300\n",
            "459/459 [==============================] - 0s 311us/sample - loss: 0.8382 - accuracy: 0.6253 - val_loss: 1.7480 - val_accuracy: 0.3005\n",
            "Epoch 278/300\n",
            "459/459 [==============================] - 0s 291us/sample - loss: 0.8366 - accuracy: 0.6253 - val_loss: 1.7506 - val_accuracy: 0.2951\n",
            "Epoch 279/300\n",
            "459/459 [==============================] - 0s 306us/sample - loss: 0.8354 - accuracy: 0.6253 - val_loss: 1.7527 - val_accuracy: 0.2951\n",
            "Epoch 280/300\n",
            "459/459 [==============================] - 0s 278us/sample - loss: 0.8334 - accuracy: 0.6275 - val_loss: 1.7543 - val_accuracy: 0.3005\n",
            "Epoch 281/300\n",
            "459/459 [==============================] - 0s 283us/sample - loss: 0.8322 - accuracy: 0.6275 - val_loss: 1.7548 - val_accuracy: 0.2951\n",
            "Epoch 282/300\n",
            "459/459 [==============================] - 0s 282us/sample - loss: 0.8307 - accuracy: 0.6318 - val_loss: 1.7601 - val_accuracy: 0.2951\n",
            "Epoch 283/300\n",
            "459/459 [==============================] - 0s 271us/sample - loss: 0.8291 - accuracy: 0.6340 - val_loss: 1.7636 - val_accuracy: 0.2951\n",
            "Epoch 284/300\n",
            "459/459 [==============================] - 0s 290us/sample - loss: 0.8279 - accuracy: 0.6318 - val_loss: 1.7660 - val_accuracy: 0.2951\n",
            "Epoch 285/300\n",
            "459/459 [==============================] - 0s 281us/sample - loss: 0.8263 - accuracy: 0.6253 - val_loss: 1.7682 - val_accuracy: 0.2951\n",
            "Epoch 286/300\n",
            "459/459 [==============================] - 0s 287us/sample - loss: 0.8250 - accuracy: 0.6275 - val_loss: 1.7720 - val_accuracy: 0.3005\n",
            "Epoch 287/300\n",
            "459/459 [==============================] - 0s 274us/sample - loss: 0.8236 - accuracy: 0.6253 - val_loss: 1.7751 - val_accuracy: 0.3060\n",
            "Epoch 288/300\n",
            "459/459 [==============================] - 0s 276us/sample - loss: 0.8222 - accuracy: 0.6253 - val_loss: 1.7768 - val_accuracy: 0.3005\n",
            "Epoch 289/300\n",
            "459/459 [==============================] - 0s 270us/sample - loss: 0.8208 - accuracy: 0.6296 - val_loss: 1.7799 - val_accuracy: 0.3005\n",
            "Epoch 290/300\n",
            "459/459 [==============================] - 0s 330us/sample - loss: 0.8198 - accuracy: 0.6318 - val_loss: 1.7819 - val_accuracy: 0.3005\n",
            "Epoch 291/300\n",
            "459/459 [==============================] - 0s 285us/sample - loss: 0.8185 - accuracy: 0.6296 - val_loss: 1.7844 - val_accuracy: 0.3060\n",
            "Epoch 292/300\n",
            "459/459 [==============================] - 0s 258us/sample - loss: 0.8169 - accuracy: 0.6318 - val_loss: 1.7880 - val_accuracy: 0.3060\n",
            "Epoch 293/300\n",
            "459/459 [==============================] - 0s 273us/sample - loss: 0.8158 - accuracy: 0.6318 - val_loss: 1.7913 - val_accuracy: 0.3005\n",
            "Epoch 294/300\n",
            "459/459 [==============================] - 0s 281us/sample - loss: 0.8144 - accuracy: 0.6340 - val_loss: 1.7918 - val_accuracy: 0.3005\n",
            "Epoch 295/300\n",
            "459/459 [==============================] - 0s 288us/sample - loss: 0.8131 - accuracy: 0.6318 - val_loss: 1.7956 - val_accuracy: 0.3060\n",
            "Epoch 296/300\n",
            "459/459 [==============================] - 0s 273us/sample - loss: 0.8119 - accuracy: 0.6340 - val_loss: 1.7997 - val_accuracy: 0.3060\n",
            "Epoch 297/300\n",
            "459/459 [==============================] - 0s 276us/sample - loss: 0.8106 - accuracy: 0.6296 - val_loss: 1.8021 - val_accuracy: 0.3005\n",
            "Epoch 298/300\n",
            "459/459 [==============================] - 0s 264us/sample - loss: 0.8095 - accuracy: 0.6318 - val_loss: 1.8044 - val_accuracy: 0.3005\n",
            "Epoch 299/300\n",
            "459/459 [==============================] - 0s 285us/sample - loss: 0.8082 - accuracy: 0.6318 - val_loss: 1.8066 - val_accuracy: 0.3005\n",
            "Epoch 300/300\n",
            "459/459 [==============================] - 0s 297us/sample - loss: 0.8070 - accuracy: 0.6318 - val_loss: 1.8080 - val_accuracy: 0.3060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SwLGxV7rrG8",
        "colab_type": "text"
      },
      "source": [
        "MODEL PERFORMANCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b-jwoaTjOy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRHVCIxIjP84",
        "colab_type": "code",
        "outputId": "f1957c33-ac0a-453a-a4ae-d97f3103d912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4dqPcXIjvyX",
        "colab_type": "code",
        "outputId": "7b6ec629-7452-4cc4-d6bc-a768e4084201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plot_graphs(history, 'loss')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlFJKudIAoZRSypUGCKWUUq7+HwwG/ZBcndogAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjwZhxfNw38y",
        "colab_type": "text"
      },
      "source": [
        "USE SINGLE LAYER LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7rOD3KWxc5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(1000, 64),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkOuU8JLxiX5",
        "colab_type": "code",
        "outputId": "5fab0402-7f87-4956-8581-6a7c7b4d275b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "model1.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          64000     \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               66048     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 138,629\n",
            "Trainable params: 138,629\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd7x8SAExkDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.compile(loss = 'sparse_categorical_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1sQZeNExqEP",
        "colab_type": "code",
        "outputId": "83b12c0e-1e18-4757-880f-ef3422268b4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "model1.fit(training_data, training_labels,\n",
        "                    validation_data=(test_data, test_labels),\n",
        "                    epochs=30, \n",
        "                    callbacks = [ callbacks])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 524 samples, validate on 130 samples\n",
            "Epoch 1/30\n",
            "524/524 [==============================] - 4s 7ms/sample - loss: 1.6114 - accuracy: 0.1698 - val_loss: 1.6081 - val_accuracy: 0.2615\n",
            "Epoch 2/30\n",
            "524/524 [==============================] - 0s 878us/sample - loss: 1.6036 - accuracy: 0.3130 - val_loss: 1.6047 - val_accuracy: 0.2462\n",
            "Epoch 3/30\n",
            "524/524 [==============================] - 0s 860us/sample - loss: 1.5803 - accuracy: 0.3989 - val_loss: 1.5904 - val_accuracy: 0.2692\n",
            "Epoch 4/30\n",
            "524/524 [==============================] - 0s 920us/sample - loss: 1.4938 - accuracy: 0.5706 - val_loss: 1.5262 - val_accuracy: 0.3154\n",
            "Epoch 5/30\n",
            "524/524 [==============================] - 0s 838us/sample - loss: 1.2412 - accuracy: 0.4981 - val_loss: 1.4824 - val_accuracy: 0.3308\n",
            "Epoch 6/30\n",
            "524/524 [==============================] - 0s 857us/sample - loss: 0.9039 - accuracy: 0.6679 - val_loss: 1.5757 - val_accuracy: 0.3231\n",
            "Epoch 7/30\n",
            "524/524 [==============================] - 0s 863us/sample - loss: 0.6619 - accuracy: 0.7977 - val_loss: 1.7498 - val_accuracy: 0.3615\n",
            "Epoch 8/30\n",
            "524/524 [==============================] - 0s 845us/sample - loss: 0.4423 - accuracy: 0.8779 - val_loss: 1.8188 - val_accuracy: 0.3769\n",
            "Epoch 9/30\n",
            "524/524 [==============================] - 0s 860us/sample - loss: 0.2893 - accuracy: 0.9370 - val_loss: 2.3818 - val_accuracy: 0.4231\n",
            "Epoch 10/30\n",
            "512/524 [============================>.] - ETA: 0s - loss: 0.1817 - accuracy: 0.9785\n",
            "Reach accuracy of 95% and stop training!\n",
            "524/524 [==============================] - 0s 826us/sample - loss: 0.1812 - accuracy: 0.9790 - val_loss: 2.3564 - val_accuracy: 0.4231\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9584ed3278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToxroTOZy3Az",
        "colab_type": "text"
      },
      "source": [
        "USE MULTIPLE LAYER LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtvqCYMny1Oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(1000, 64),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly8VAdY0y9FE",
        "colab_type": "code",
        "outputId": "6f95882b-c96d-4722-e957-ab293d49c8ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 64)          64000     \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, None, 128)         66048     \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 64)                41216     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 175,749\n",
            "Trainable params: 175,749\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IQivKPJzIeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.compile(loss = 'sparse_categorical_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__h1ykqqy_Iz",
        "colab_type": "code",
        "outputId": "1e4ff5df-1b17-4f49-a94b-40e866367092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "model2.fit(training_data, training_labels,\n",
        "           validation_data=(test_data, test_labels),\n",
        "           epochs=30, \n",
        "           callbacks = [ callbacks])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 524 samples, validate on 130 samples\n",
            "Epoch 1/30\n",
            "524/524 [==============================] - 7s 13ms/sample - loss: 1.6113 - accuracy: 0.1832 - val_loss: 1.6093 - val_accuracy: 0.1538\n",
            "Epoch 2/30\n",
            "524/524 [==============================] - 1s 2ms/sample - loss: 1.6056 - accuracy: 0.2195 - val_loss: 1.6075 - val_accuracy: 0.2000\n",
            "Epoch 3/30\n",
            "524/524 [==============================] - 1s 2ms/sample - loss: 1.5872 - accuracy: 0.3607 - val_loss: 1.5987 - val_accuracy: 0.2692\n",
            "Epoch 4/30\n",
            "524/524 [==============================] - 1s 1ms/sample - loss: 1.3670 - accuracy: 0.5458 - val_loss: 1.8439 - val_accuracy: 0.2769\n",
            "Epoch 5/30\n",
            "524/524 [==============================] - 1s 2ms/sample - loss: 0.7848 - accuracy: 0.6927 - val_loss: 2.0417 - val_accuracy: 0.3308\n",
            "Epoch 6/30\n",
            "524/524 [==============================] - 1s 2ms/sample - loss: 0.3752 - accuracy: 0.8912 - val_loss: 2.7186 - val_accuracy: 0.3385\n",
            "Epoch 7/30\n",
            "512/524 [============================>.] - ETA: 0s - loss: 0.1774 - accuracy: 0.9590\n",
            "Reach accuracy of 95% and stop training!\n",
            "524/524 [==============================] - 1s 2ms/sample - loss: 0.1767 - accuracy: 0.9580 - val_loss: 3.1595 - val_accuracy: 0.3846\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f957a6aef98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgzBEyX2zb_V",
        "colab_type": "text"
      },
      "source": [
        "USE GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMer-IAAzeCs",
        "colab_type": "code",
        "outputId": "d2f4a101-fdac-426a-9999-fb37ea16d3bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "model3 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(1000, 64, input_length=24),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "model3.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model3.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 24, 64)            64000     \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 64)                18816     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 6)                 390       \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 5)                 35        \n",
            "=================================================================\n",
            "Total params: 83,241\n",
            "Trainable params: 83,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BspZTqB7z9jY",
        "colab_type": "code",
        "outputId": "f4cd6942-b7aa-46c7-e099-8e8151855f97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "model3.fit(training_data, training_labels,\n",
        "                    validation_data=(test_data, test_labels),\n",
        "                    epochs=30, \n",
        "                    callbacks = [ callbacks])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 524 samples, validate on 130 samples\n",
            "Epoch 1/30\n",
            "524/524 [==============================] - 4s 7ms/sample - loss: 1.6102 - accuracy: 0.2099 - val_loss: 1.6057 - val_accuracy: 0.2462\n",
            "Epoch 2/30\n",
            "524/524 [==============================] - 0s 662us/sample - loss: 1.5952 - accuracy: 0.2538 - val_loss: 1.6034 - val_accuracy: 0.2000\n",
            "Epoch 3/30\n",
            "524/524 [==============================] - 0s 628us/sample - loss: 1.5684 - accuracy: 0.2309 - val_loss: 1.5907 - val_accuracy: 0.2308\n",
            "Epoch 4/30\n",
            "524/524 [==============================] - 0s 686us/sample - loss: 1.5167 - accuracy: 0.2653 - val_loss: 1.5860 - val_accuracy: 0.2231\n",
            "Epoch 5/30\n",
            "524/524 [==============================] - 0s 658us/sample - loss: 1.4132 - accuracy: 0.3588 - val_loss: 1.6173 - val_accuracy: 0.3231\n",
            "Epoch 6/30\n",
            "524/524 [==============================] - 0s 630us/sample - loss: 1.2616 - accuracy: 0.4866 - val_loss: 1.5625 - val_accuracy: 0.3538\n",
            "Epoch 7/30\n",
            "524/524 [==============================] - 0s 637us/sample - loss: 1.0967 - accuracy: 0.6069 - val_loss: 1.7247 - val_accuracy: 0.3154\n",
            "Epoch 8/30\n",
            "524/524 [==============================] - 0s 674us/sample - loss: 0.9403 - accuracy: 0.6947 - val_loss: 1.7436 - val_accuracy: 0.3000\n",
            "Epoch 9/30\n",
            "524/524 [==============================] - 0s 642us/sample - loss: 0.7923 - accuracy: 0.8053 - val_loss: 1.7286 - val_accuracy: 0.3231\n",
            "Epoch 10/30\n",
            "524/524 [==============================] - 0s 636us/sample - loss: 0.6634 - accuracy: 0.8492 - val_loss: 1.8570 - val_accuracy: 0.3000\n",
            "Epoch 11/30\n",
            "524/524 [==============================] - 0s 623us/sample - loss: 0.4920 - accuracy: 0.8989 - val_loss: 1.8777 - val_accuracy: 0.3231\n",
            "Epoch 12/30\n",
            "512/524 [============================>.] - ETA: 0s - loss: 0.3568 - accuracy: 0.9492\n",
            "Reach accuracy of 95% and stop training!\n",
            "524/524 [==============================] - 0s 640us/sample - loss: 0.3551 - accuracy: 0.9504 - val_loss: 1.9236 - val_accuracy: 0.3538\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9580861c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMXrnsq3vKbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clear the trained models\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}